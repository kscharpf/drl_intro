{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs, seed):\n",
    "        super(DQN, self).__init__()\n",
    "        print(\"h: {} w: {} outputs {}\".format(h, w, outputs))\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(w,kernel_size=8,stride=4), kernel_size=4, stride=2)\n",
    "        convh = conv2d_size_out(conv2d_size_out(h, kernel_size=8, stride=4), \n",
    "                                                kernel_size=4, stride=2)\n",
    "        linear_input_size = convw * convh * 64\n",
    "        self.fc1 = nn.Linear(linear_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512,128)\n",
    "        self.fc3 = nn.Linear(128, outputs)\n",
    "        \n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = int(2e5)\n",
    "TAU = 0.01\n",
    "GAMMA = 0.99\n",
    "LR=1e-4\n",
    "class DQNAgent:\n",
    "    def __init__(self, height, width, action_size, seed):\n",
    "        self.seed = random.seed(seed)\n",
    "        self.action_size = action_size\n",
    "        self.batch_indices = torch.arange(BATCH_SIZE).long().to(device)\n",
    "        self.samples_before_learning = 10000\n",
    "        self.learn_interval = 20\n",
    "        self.parameter_update_interval = 2\n",
    "        self.tau = TAU\n",
    "        self.gamma = GAMMA\n",
    "\n",
    "        self.qnetwork_local = DQN(height, width, action_size, seed).to(device)\n",
    "        self.qnetwork_target = DQN(height, width, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def act(self, state, eps=0.):\n",
    "    \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        state = state.reshape((1,state.shape[0], state.shape[1], state.shape[2]))\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        if random.random() < eps:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        else:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        state = state.reshape((1,1,state.shape[0], state.shape[1]))\n",
    "        next_state = next_state.reshape((1,1,next_state.shape[0], next_state.shape[1]))\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.t_step += 1\n",
    "        if self.t_step % self.learn_interval == 0:\n",
    "            if len(self.memory) > self.samples_before_learning:\n",
    "                #state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "                state = torch.from_numpy(state).float().to(device)\n",
    "\n",
    "                #next_state = torch.from_numpy(next_state).float().unsqueeze(0).to(device)\n",
    "                next_state = torch.from_numpy(next_state).float().to(device)\n",
    "\n",
    "                target = self.qnetwork_local(state).data\n",
    "                old_val = target[0][action]\n",
    "                target_val = self.qnetwork_target(next_state).data\n",
    "                if done:\n",
    "                    target[0][action] = reward\n",
    "                else:\n",
    "                    target[0][action] = reward + self.gamma * torch.max(target_val)\n",
    "                indices=None\n",
    "                weights=None\n",
    "                states, actions, rewards, next_states, dones = self.memory.sample()\n",
    "\n",
    "                self.learn(states, actions, rewards, next_states, dones, indices, weights, self.gamma)\n",
    "        \n",
    "    def learn(self, states, actions, rewards, next_states, dones, indices, weights, gamma):\n",
    "        states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack(dones)).float().to(device)\n",
    "        states = states.reshape((BATCH_SIZE, 1, states.shape[1], states.shape[2]))\n",
    "        next_states = next_states.reshape((BATCH_SIZE, 1, next_states.shape[1], next_states.shape[2]))\n",
    "\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach()\n",
    "\n",
    "        Q_targets_next = Q_targets_next.max(1)[0]\n",
    "\n",
    "        Q_targets = rewards + gamma * Q_targets_next.reshape((BATCH_SIZE, 1)) * (1 - dones)\n",
    "\n",
    "        pred = self.qnetwork_local(states)\n",
    "        Q_expected = pred.gather(1, actions)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.t_step % self.parameter_update_interval == 0:\n",
    "            self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
    "\n",
    "    def soft_update(self, qnetwork_local, qnetwork_target, tau):\n",
    "        for local_param, target_param in zip(qnetwork_local.parameters(), qnetwork_target.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grayscale(img):\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    return to_grayscale(downsample(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 105 w: 80 outputs 4\n",
      "h: 105 w: 80 outputs 4\n",
      "Game  0 Completed in 2001 steps with score 9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "try:\n",
    "    env\n",
    "except:\n",
    "    env = None\n",
    "if env:\n",
    "    env.close()\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()\n",
    "\n",
    "\n",
    "scores = []\n",
    "import gym\n",
    "import time\n",
    "env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "action_size = env.action_space.n\n",
    "height, width = preprocess(state).shape\n",
    "agent = DQNAgent(height, width, action_size, 0)\n",
    "agent.qnetwork_local.load_state_dict(torch.load('breakout_5000.pt'))\n",
    "for game in range(1):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    state = preprocess(state)\n",
    "    score = 0\n",
    "    step = 0\n",
    "    while step < 2000:\n",
    "        env.render()\n",
    "        action = agent.act(state, 0.)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        state = preprocess(next_state)\n",
    "        time.sleep(0.01)\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "        step += 1\n",
    "    print(\"Game {:2d} Completed in {:4d} steps with score {}\".format(game, step+1, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
