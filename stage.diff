diff --git a/.ipynb_checkpoints/DRLIntro-checkpoint.ipynb b/.ipynb_checkpoints/DRLIntro-checkpoint.ipynb
deleted file mode 100644
index d87d4b8..0000000
--- a/.ipynb_checkpoints/DRLIntro-checkpoint.ipynb
+++ /dev/null
@@ -1,1465 +0,0 @@
-{
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": 2,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Collecting gym\n",
-      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/75/9e841bc2bc75128e0b65c3d5255d0bd16becb9d8f7120b965d41b8e70041/gym-0.14.0.tar.gz (1.6MB)\n",
-      "\u001b[K     |████████████████████████████████| 1.6MB 1.5MB/s eta 0:00:01\n",
-      "\u001b[?25hRequirement already satisfied: scipy in /home/kevin/anaconda3/envs/torchdrl/lib/python3.7/site-packages (from gym) (1.3.1)\n",
-      "Requirement already satisfied: numpy>=1.10.4 in /home/kevin/anaconda3/envs/torchdrl/lib/python3.7/site-packages (from gym) (1.16.5)\n",
-      "Requirement already satisfied: six in /home/kevin/anaconda3/envs/torchdrl/lib/python3.7/site-packages (from gym) (1.12.0)\n",
-      "Collecting pyglet<=1.3.2,>=1.2.0 (from gym)\n",
-      "  Using cached https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl\n",
-      "Collecting cloudpickle~=1.2.0 (from gym)\n",
-      "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
-      "Collecting future (from pyglet<=1.3.2,>=1.2.0->gym)\n",
-      "  Using cached https://files.pythonhosted.org/packages/90/52/e20466b85000a181e1e144fd8305caf2cf475e2f9674e797b222f8105f5f/future-0.17.1.tar.gz\n",
-      "Building wheels for collected packages: gym, future\n",
-      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
-      "\u001b[?25h  Created wheel for gym: filename=gym-0.14.0-cp37-none-any.whl size=1637526 sha256=8ec89bed012210e268189bef254a1fa86d1aa187685a5add8acc5fac19b8354d\n",
-      "  Stored in directory: /home/kevin/.cache/pip/wheels/7e/53/f6/c0cd3c9bf953f35c0aee7fa62ea209371e92f5e5cced3245ba\n",
-      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
-      "\u001b[?25h  Created wheel for future: filename=future-0.17.1-cp37-none-any.whl size=488730 sha256=fdf5aea76a1ef8ecae79760f976b9c55d13dddb08cb2f4a199ced16471ab9f06\n",
-      "  Stored in directory: /home/kevin/.cache/pip/wheels/0c/61/d2/d6b7317325828fbb39ee6ad559dbe4664d0896da4721bf379e\n",
-      "Successfully built gym future\n",
-      "Installing collected packages: future, pyglet, cloudpickle, gym\n",
-      "Successfully installed cloudpickle-1.2.2 future-0.17.1 gym-0.14.0 pyglet-1.3.2\n"
-     ]
-    }
-   ],
-   "source": [
-    "!pip install gym"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 1,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import gym\n",
-    "import math\n",
-    "import random\n",
-    "import numpy as np\n",
-    "import matplotlib\n",
-    "import matplotlib.pyplot as plt\n",
-    "from collections import namedtuple\n",
-    "from itertools import count\n",
-    "from PIL import Image\n",
-    "\n",
-    "import torch\n",
-    "import torch.nn as nn\n",
-    "import torch.optim as optim\n",
-    "import torch.nn.functional as F\n",
-    "import torchvision.transforms as T\n",
-    "\n",
-    "\n",
-    "env = gym.make('CartPole-v0').unwrapped\n",
-    "\n",
-    "# set up matplotlib\n",
-    "is_ipython = 'inline' in matplotlib.get_backend()\n",
-    "if is_ipython:\n",
-    "    from IPython import display\n",
-    "\n",
-    "plt.ion()\n",
-    "\n",
-    "# if gpu is to be used\n",
-    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 2,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "Transition = namedtuple('Transition',\n",
-    "                        ('state', 'action', 'next_state', 'reward'))\n",
-    "\n",
-    "\n",
-    "class ReplayMemory(object):\n",
-    "\n",
-    "    def __init__(self, capacity):\n",
-    "        self.capacity = capacity\n",
-    "        self.memory = []\n",
-    "        self.position = 0\n",
-    "\n",
-    "    def push(self, *args):\n",
-    "        \"\"\"Saves a transition.\"\"\"\n",
-    "        if len(self.memory) < self.capacity:\n",
-    "            self.memory.append(None)\n",
-    "        self.memory[self.position] = Transition(*args)\n",
-    "        self.position = (self.position + 1) % self.capacity\n",
-    "\n",
-    "    def sample(self, batch_size):\n",
-    "        return random.sample(self.memory, batch_size)\n",
-    "\n",
-    "    def __len__(self):\n",
-    "        return len(self.memory)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 3,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "class DQN(nn.Module):\n",
-    "\n",
-    "    def __init__(self, h, w, outputs):\n",
-    "        super(DQN, self).__init__()\n",
-    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
-    "        self.bn1 = nn.BatchNorm2d(16)\n",
-    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
-    "        self.bn2 = nn.BatchNorm2d(32)\n",
-    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
-    "        self.bn3 = nn.BatchNorm2d(32)\n",
-    "\n",
-    "        # Number of Linear input connections depends on output of conv2d layers\n",
-    "        # and therefore the input image size, so compute it.\n",
-    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
-    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
-    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
-    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
-    "        linear_input_size = convw * convh * 32\n",
-    "        self.head = nn.Linear(linear_input_size, outputs)\n",
-    "\n",
-    "    # Called with either one element to determine next action, or a batch\n",
-    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
-    "    def forward(self, x):\n",
-    "        x = F.relu(self.bn1(self.conv1(x)))\n",
-    "        x = F.relu(self.bn2(self.conv2(x)))\n",
-    "        x = F.relu(self.bn3(self.conv3(x)))\n",
-    "        return self.head(x.view(x.size(0), -1))"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 4,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "image/png": "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\n",
-      "text/plain": [
-       "<Figure size 432x288 with 1 Axes>"
-      ]
-     },
-     "metadata": {
-      "needs_background": "light"
-     },
-     "output_type": "display_data"
-    }
-   ],
-   "source": [
-    "resize = T.Compose([T.ToPILImage(),\n",
-    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
-    "                    T.ToTensor()])\n",
-    "\n",
-    "\n",
-    "def get_cart_location(screen_width):\n",
-    "    world_width = env.x_threshold * 2\n",
-    "    scale = screen_width / world_width\n",
-    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
-    "\n",
-    "def get_screen():\n",
-    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
-    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
-    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
-    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
-    "    _, screen_height, screen_width = screen.shape\n",
-    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
-    "    view_width = int(screen_width * 0.6)\n",
-    "    cart_location = get_cart_location(screen_width)\n",
-    "    if cart_location < view_width // 2:\n",
-    "        slice_range = slice(view_width)\n",
-    "    elif cart_location > (screen_width - view_width // 2):\n",
-    "        slice_range = slice(-view_width, None)\n",
-    "    else:\n",
-    "        slice_range = slice(cart_location - view_width // 2,\n",
-    "                            cart_location + view_width // 2)\n",
-    "    # Strip off the edges, so that we have a square image centered on a cart\n",
-    "    screen = screen[:, :, slice_range]\n",
-    "    # Convert to float, rescale, convert to torch tensor\n",
-    "    # (this doesn't require a copy)\n",
-    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
-    "    screen = torch.from_numpy(screen)\n",
-    "    # Resize, and add a batch dimension (BCHW)\n",
-    "    return resize(screen).unsqueeze(0).to(device)\n",
-    "\n",
-    "\n",
-    "env.reset()\n",
-    "plt.figure()\n",
-    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
-    "           interpolation='none')\n",
-    "plt.title('Example extracted screen')\n",
-    "plt.show()\n"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 5,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "BATCH_SIZE = 128\n",
-    "GAMMA = 0.999\n",
-    "EPS_START = 0.9\n",
-    "EPS_END = 0.05\n",
-    "EPS_DECAY = 200\n",
-    "TARGET_UPDATE = 10\n",
-    "\n",
-    "# Get screen size so that we can initialize layers correctly based on shape\n",
-    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
-    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
-    "init_screen = get_screen()\n",
-    "_, _, screen_height, screen_width = init_screen.shape\n",
-    "\n",
-    "# Get number of actions from gym action space\n",
-    "n_actions = env.action_space.n\n",
-    "\n",
-    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
-    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
-    "target_net.load_state_dict(policy_net.state_dict())\n",
-    "target_net.eval()\n",
-    "\n",
-    "optimizer = optim.RMSprop(policy_net.parameters())\n",
-    "memory = ReplayMemory(10000)\n",
-    "\n",
-    "\n",
-    "steps_done = 0\n",
-    "\n",
-    "\n",
-    "def select_action(state):\n",
-    "    global steps_done\n",
-    "    sample = random.random()\n",
-    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
-    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
-    "    steps_done += 1\n",
-    "    if sample > eps_threshold:\n",
-    "        with torch.no_grad():\n",
-    "            # t.max(1) will return largest column value of each row.\n",
-    "            # second column on max result is index of where max element was\n",
-    "            # found, so we pick action with the larger expected reward.\n",
-    "            return policy_net(state).max(1)[1].view(1, 1)\n",
-    "    else:\n",
-    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
-    "\n",
-    "\n",
-    "episode_durations = []\n",
-    "\n",
-    "\n",
-    "def plot_durations():\n",
-    "    plt.figure(2)\n",
-    "    plt.clf()\n",
-    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
-    "    plt.title('Training...')\n",
-    "    plt.xlabel('Episode')\n",
-    "    plt.ylabel('Duration')\n",
-    "    plt.plot(durations_t.numpy())\n",
-    "    # Take 100 episode averages and plot them too\n",
-    "    if len(durations_t) >= 100:\n",
-    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
-    "        means = torch.cat((torch.zeros(99), means))\n",
-    "        plt.plot(means.numpy())\n",
-    "\n",
-    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
-    "    if is_ipython:\n",
-    "        display.clear_output(wait=True)\n",
-    "        display.display(plt.gcf())"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 6,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "def optimize_model():\n",
-    "    if len(memory) < BATCH_SIZE:\n",
-    "        return\n",
-    "    transitions = memory.sample(BATCH_SIZE)\n",
-    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
-    "    # detailed explanation). This converts batch-array of Transitions\n",
-    "    # to Transition of batch-arrays.\n",
-    "    batch = Transition(*zip(*transitions))\n",
-    "\n",
-    "    # Compute a mask of non-final states and concatenate the batch elements\n",
-    "    # (a final state would've been the one after which simulation ended)\n",
-    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
-    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
-    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
-    "                                                if s is not None])\n",
-    "    state_batch = torch.cat(batch.state)\n",
-    "    action_batch = torch.cat(batch.action)\n",
-    "    reward_batch = torch.cat(batch.reward)\n",
-    "\n",
-    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
-    "    # columns of actions taken. These are the actions which would've been taken\n",
-    "    # for each batch state according to policy_net\n",
-    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
-    "\n",
-    "    # Compute V(s_{t+1}) for all next states.\n",
-    "    # Expected values of actions for non_final_next_states are computed based\n",
-    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
-    "    # This is merged based on the mask, such that we'll have either the expected\n",
-    "    # state value or 0 in case the state was final.\n",
-    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
-    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
-    "    # Compute the expected Q values\n",
-    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
-    "\n",
-    "    # Compute Huber loss\n",
-    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
-    "\n",
-    "    # Optimize the model\n",
-    "    optimizer.zero_grad()\n",
-    "    loss.backward()\n",
-    "    for param in policy_net.parameters():\n",
-    "        param.grad.data.clamp_(-1, 1)\n",
-    "    optimizer.step()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 9,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "<Figure size 432x288 with 0 Axes>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Complete\n"
-     ]
-    },
-    {
-     "data": {
-      "text/plain": [
-       "<Figure size 432x288 with 0 Axes>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
-   "source": [
-    "num_episodes = 500\n",
-    "for i_episode in range(num_episodes):\n",
-    "    # Initialize the environment and state\n",
-    "    env.reset()\n",
-    "    last_screen = get_screen()\n",
-    "    current_screen = get_screen()\n",
-    "    state = current_screen - last_screen\n",
-    "    for t in count():\n",
-    "        # Select and perform an action\n",
-    "        action = select_action(state)\n",
-    "        _, reward, done, _ = env.step(action.item())\n",
-    "        reward = torch.tensor([reward], device=device)\n",
-    "\n",
-    "        # Observe new state\n",
-    "        last_screen = current_screen\n",
-    "        current_screen = get_screen()\n",
-    "        if not done:\n",
-    "            next_state = current_screen - last_screen\n",
-    "        else:\n",
-    "            next_state = None\n",
-    "\n",
-    "        # Store the transition in memory\n",
-    "        memory.push(state, action, next_state, reward)\n",
-    "\n",
-    "        # Move to the next state\n",
-    "        state = next_state\n",
-    "\n",
-    "        # Perform one step of the optimization (on the target network)\n",
-    "        optimize_model()\n",
-    "        if done:\n",
-    "            episode_durations.append(t + 1)\n",
-    "            plot_durations()\n",
-    "            break\n",
-    "    # Update the target network, copying all weights and biases in DQN\n",
-    "    if i_episode % TARGET_UPDATE == 0:\n",
-    "        target_net.load_state_dict(policy_net.state_dict())\n",
-    "\n",
-    "print('Complete')\n",
-    "env.render()\n",
-    "env.close()\n",
-    "plt.ioff()\n",
-    "plt.show()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 10,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "tensor([1.])"
-      ]
-     },
-     "execution_count": 10,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "reward"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 4,
-   "metadata": {},
-   "outputs": [
-    {
-     "ename": "NameError",
-     "evalue": "name 'matplotlib' is not defined",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
-      "\u001b[0;32m<ipython-input-4-1218fea4a243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# set up matplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mis_ipython\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'inline'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_ipython\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;31mNameError\u001b[0m: name 'matplotlib' is not defined"
-     ]
-    }
-   ],
-   "source": [
-    "import time\n",
-    "import gym\n",
-    "try:\n",
-    "    env\n",
-    "except: \n",
-    "    env = None\n",
-    "if env:\n",
-    "    env.close()\n",
-    "env = gym.make('CartPole-v0').unwrapped\n",
-    "\n",
-    "# set up matplotlib\n",
-    "is_ipython = 'inline' in matplotlib.get_backend()\n",
-    "if is_ipython:\n",
-    "    from IPython import display\n",
-    "\n",
-    "plt.ion()\n",
-    "\n",
-    "# if gpu is to be used\n",
-    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
-    "env.reset()\n",
-    "last_screen = get_screen()\n",
-    "current_screen = get_screen()\n",
-    "state = current_screen - last_screen\n",
-    "for i in range(300):\n",
-    "    env.render()\n",
-    "    time.sleep(0.1)\n",
-    "    # Select and perform an action\n",
-    "    action = select_action(state)\n",
-    "    _, reward, done, _ = env.step(action.item())\n",
-    "    \n",
-    "    print(\"i: {} reward {}\".format(i, reward))\n",
-    "    reward = torch.tensor([reward], device=device)\n",
-    "\n",
-    "    # Observe new state\n",
-    "    last_screen = current_screen\n",
-    "    current_screen = get_screen()\n",
-    "    \n",
-    "    if done:\n",
-    "        print(\"DONE\")\n",
-    "        break\n",
-    "    next_state = current_screen - last_screen\n",
-    "\n",
-    "    # Move to the next state\n",
-    "    state = next_state\n"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 36,
-   "metadata": {},
-   "outputs": [
-    {
-     "ename": "ValueError",
-     "evalue": "Images of type float must be between -1 and 1.",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
-      "\u001b[0;32m<ipython-input-36-f014bd131009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astronaut.png'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mastronaut_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astronaut_gray.png'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgray_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astronaut_edges.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
-      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, plugin, check_contrast, **plugin_args)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s is a boolean image: setting True to 1 and False to 0'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imsave'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m                                (plugin, kind))\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, format_str, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid number of channels in image array.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndarray_to_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\u001b[0m in \u001b[0;36mndarray_to_pil\u001b[0;34m(arr, format_str)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'f'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_as_uint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/util/dtype.py\u001b[0m in \u001b[0;36mimg_as_uint\u001b[0;34m(image, force_copy)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[0;32m--> 437\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/util/dtype.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(image, dtype, force_copy, uniform)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Images of type float must be between -1 and 1.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;31m# floating point -> integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mprec_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;31mValueError\u001b[0m: Images of type float must be between -1 and 1."
-     ]
-    }
-   ],
-   "source": [
-    "from sklearn.datasets import load_sample_images\n",
-    "from skimage.viewer import ImageViewer\n",
-    "from skimage import data, color, io\n",
-    "import numpy as np\n",
-    "\n",
-    "# suppress warnings\n",
-    "def warn(*args, **kwargs):\n",
-    "    pass\n",
-    "import warnings\n",
-    "warnings.warn = warn\n",
-    "\n",
-    "\n",
-    "# convolution code derived from \n",
-    "# http://machinelearninguru.com/computer_vision/basics/convolution/image_convolution_1.html\n",
-    "def convolve(img, kernel):    \n",
-    "    \"\"\"assume 3x3 kernel shape\"\"\"\n",
-    "    # construct an output array of the same size as the input image\n",
-    "    output = np.zeros_like(img)\n",
-    "    \n",
-    "    # create a padded image so that we can convolve through the whole thing, \"same\" padding\n",
-    "    image_padded = np.zeros((img.shape[0] + kernel.shape[0]-1, img.shape[1]+kernel.shape[1]-1))\n",
-    "    image_padded[1:-1,1:-1] = img\n",
-    "    for x in range(img.shape[1]):\n",
-    "        for y in range(img.shape[0]):\n",
-    "            output[y,x] = (kernel * image_padded[y:y+3,x:x+3]).sum()\n",
-    "    return output\n",
-    "\n",
-    "def preprocess(img):\n",
-    "    return color.rgb2gray(img)\n",
-    "\n",
-    "astronaut_image = data.astronaut()\n",
-    "gray_img = preprocess(astronaut_image)\n",
-    "# edge detection kernel: \n",
-    "edge_img = convolve(gray_img, np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]]))\n",
-    "\n",
-    "io.imsave('astronaut.png',astronaut_image)\n",
-    "io.imsave('astronaut_gray.png',gray_img)\n",
-    "io.imsave('astronaut_edges.png', edge_img)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 47,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "array([[1, 2, 3],\n",
-       "       [4, 5, 6],\n",
-       "       [7, 8, 9]])"
-      ]
-     },
-     "execution_count": 47,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "img = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
-    "img\n"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 48,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "array([[1, 0],\n",
-       "       [0, 1]])"
-      ]
-     },
-     "execution_count": 48,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "kernel = np.array([[1,0],[0,1]])\n",
-    "kernel"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 50,
-   "metadata": {},
-   "outputs": [
-    {
-     "ename": "IndexError",
-     "evalue": "index 3 is out of bounds for axis 1 with size 3",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
-      "\u001b[0;32m<ipython-input-50-13d1ec0cb0db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 1 with size 3"
-     ]
-    }
-   ],
-   "source": [
-    "O = np.zeros_like(img)\n",
-    "m = 0\n",
-    "n = 0\n",
-    "for m in range(img.shape[0]):\n",
-    "    for n in range(img.shape[1]):\n",
-    "        for i in range(kernel.shape[0]):\n",
-    "            for j in range(kernel.shape[1]):\n",
-    "                O[i,j] += kernel[i,j] * img[i+m,j+n]\n",
-    "O"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 52,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "array([[-1, -1, -1],\n",
-       "       [-1,  8, -1],\n",
-       "       [-1, -1, -1]])"
-      ]
-     },
-     "execution_count": 52,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "edge_filter = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])\n",
-    "edge_filter"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 53,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "0"
-      ]
-     },
-     "execution_count": 53,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "I = np.array([[1,1,1],[1,1,1],[1,1,1]])\n",
-    "(I * edge_filter).sum()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 19,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "right_edge_filter = np.flipud(np.fliplr(right_edge_filter))"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 20,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "output = np.zeros_like(astronaut_image)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 54,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "(512, 512, 3)"
-      ]
-     },
-     "execution_count": 54,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "output.shape"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 55,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "512.0\n"
-     ]
-    }
-   ],
-   "source": [
-    "def conv_output_size(input_dim, padding, stride, kernel_dim):\n",
-    "    P = 1 if padding == 'same' else 0\n",
-    "    return (input_dim - kernel_dim + P*2)/stride + 1\n",
-    "print(conv_output_size(512,'same',1,3))\n",
-    "    "
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 51,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "from skimage import color\n",
-    "gray_image = color.rgb2gray(astronaut_image)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 33,
-   "metadata": {},
-   "outputs": [
-    {
-     "ename": "FileNotFoundError",
-     "evalue": "[Errno 2] No such file or directory: 'astronaut_gray.bmp'",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
-      "\u001b[0;32m<ipython-input-33-3f3b0d2b4a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgray_image2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astronaut_gray.bmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray_image2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, flatten, **plugin_args)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m                                (plugin, kind))\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, dtype, img_num, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \"\"\"\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpil_to_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'astronaut_gray.bmp'"
-     ]
-    }
-   ],
-   "source": [
-    "gray_image2 = io.imread('astronaut_gray.bmp')\n",
-    "viewer = ImageViewer(gray_image2)\n",
-    "viewer.show()\n"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 23,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "image_padded = np.zeros((gray_image.shape[0] + 2, gray_image.shape[1]+2))"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 24,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "image_padded[1:-1,1:-1] = gray_image"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 32,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/home/kevin/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/_io.py:141: UserWarning: astronaut_edges.png is a low contrast image\n",
-      "  warn('%s is a low contrast image' % fname)\n"
-     ]
-    }
-   ],
-   "source": [
-    "for x in range(gray_image.shape[1]):\n",
-    "    for y in range(gray_image.shape[0]):\n",
-    "        output[y,x] = (right_edge_filter * image_padded[y:y+3,x:x+3]).sum()\n",
-    "        \n",
-    "io.imsave('astronaut_edges.png', output)\n",
-    "        "
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 26,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "viewer = ImageViewer(output)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 27,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "[]"
-      ]
-     },
-     "execution_count": 27,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "viewer.show()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 4,
-   "metadata": {},
-   "outputs": [
-    {
-     "ename": "ModuleNotFoundError",
-     "evalue": "No module named 'skimage'",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
-      "\u001b[0;32m<ipython-input-4-3ccc102dc542>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageViewer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
-      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
-     ]
-    }
-   ],
-   "source": [
-    "from skimage.viewer import ImageViewer"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 56,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import gym\n",
-    "import math\n",
-    "import random\n",
-    "import numpy as np\n",
-    "import matplotlib\n",
-    "import matplotlib.pyplot as plt\n",
-    "from collections import namedtuple\n",
-    "from itertools import count\n",
-    "from PIL import Image\n",
-    "\n",
-    "import torch\n",
-    "import torch.nn as nn\n",
-    "import torch.optim as optim\n",
-    "import torch.nn.functional as F\n",
-    "import torchvision.transforms as T\n",
-    "\n",
-    "\n",
-    "env = gym.make('CartPole-v0').unwrapped\n",
-    "\n",
-    "# set up matplotlib\n",
-    "is_ipython = 'inline' in matplotlib.get_backend()\n",
-    "if is_ipython:\n",
-    "    from IPython import display\n",
-    "\n",
-    "plt.ion()\n",
-    "\n",
-    "# if gpu is to be used\n",
-    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 1,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "from collections import deque, namedtuple\n",
-    "import gym\n",
-    "import math\n",
-    "import random\n",
-    "import numpy as np\n",
-    "import matplotlib\n",
-    "import matplotlib.pyplot as plt\n",
-    "from collections import namedtuple\n",
-    "from itertools import count\n",
-    "from PIL import Image\n",
-    "\n",
-    "import torch\n",
-    "import torch.nn as nn\n",
-    "import torch.optim as optim\n",
-    "import torch.nn.functional as F\n",
-    "import torchvision.transforms as T\n",
-    "class ReplayBuffer:\n",
-    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
-    "\n",
-    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
-    "        \"\"\"Initialize a ReplayBuffer object.\n",
-    "        Params\n",
-    "        ======\n",
-    "            buffer_size (int): maximum size of buffer\n",
-    "            batch_size (int): size of each training batch\n",
-    "        \"\"\"\n",
-    "        self.action_size = action_size\n",
-    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
-    "        self.batch_size = batch_size\n",
-    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
-    "        self.seed = random.seed(seed)\n",
-    "    \n",
-    "    def add(self, state, action, reward, next_state, done):\n",
-    "        \"\"\"Add a new experience to memory.\"\"\"\n",
-    "        e = self.experience(state, action, reward, next_state, done)\n",
-    "        self.memory.append(e)\n",
-    "    \n",
-    "    def sample(self):\n",
-    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
-    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
-    "\n",
-    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
-    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
-    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
-    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
-    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
-    "\n",
-    "        return (states, actions, rewards, next_states, dones)\n",
-    "\n",
-    "    def __len__(self):\n",
-    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
-    "        return len(self.memory)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 2,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "class DQN(nn.Module):\n",
-    "\n",
-    "    def __init__(self, h, w, outputs, seed):\n",
-    "        super(DQN, self).__init__()\n",
-    "        print(\"h: {} w: {} outputs {}\".format(h, w, outputs))\n",
-    "        self.seed = torch.manual_seed(seed)\n",
-    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=2)\n",
-    "        self.bn1 = nn.BatchNorm2d(32)\n",
-    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=2)\n",
-    "        self.bn2 = nn.BatchNorm2d(64)\n",
-    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
-    "        self.bn3 = nn.BatchNorm2d(32)\n",
-    "\n",
-    "        # Number of Linear input connections depends on output of conv2d layers\n",
-    "        # and therefore the input image size, so compute it.\n",
-    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
-    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
-    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
-    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
-    "        linear_input_size = convw * convh * 32\n",
-    "        self.head = nn.Linear(linear_input_size, outputs)\n",
-    "\n",
-    "    # Called with either one element to determine next action, or a batch\n",
-    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
-    "    def forward(self, x):\n",
-    "        x = F.relu(self.bn1(self.conv1(x)))\n",
-    "        x = F.relu(self.bn2(self.conv2(x)))\n",
-    "        x = F.relu(self.bn3(self.conv3(x)))\n",
-    "        return self.head(x.view(x.size(0), -1))"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 3,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import random\n",
-    "BATCH_SIZE = 128\n",
-    "BUFFER_SIZE = int(1e5)\n",
-    "TAU = 0.01\n",
-    "GAMMA = 0.99\n",
-    "LR=1e-3\n",
-    "class DQNAgent:\n",
-    "    def __init__(self, height, width, action_size, seed):\n",
-    "        self.seed = random.seed(seed)\n",
-    "        self.action_size = action_size\n",
-    "        self.batch_indices = torch.arange(BATCH_SIZE).long().to(device)\n",
-    "        self.samples_before_learning = 1000\n",
-    "        self.learn_interval = 10\n",
-    "        self.parameter_update_interval = 4\n",
-    "        self.tau = TAU\n",
-    "        self.gamma = GAMMA\n",
-    "\n",
-    "        self.qnetwork_local = DQN(height, width, action_size, seed).to(device)\n",
-    "        self.qnetwork_target = DQN(height, width, action_size, seed).to(device)\n",
-    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
-    "\n",
-    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
-    "\n",
-    "        self.t_step = 0\n",
-    "        \n",
-    "    def act(self, state, eps=0.):\n",
-    "    \n",
-    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
-    "        state = state.reshape((1,state.shape[0], state.shape[1], state.shape[2]))\n",
-    "        self.qnetwork_local.eval()\n",
-    "        with torch.no_grad():\n",
-    "            action_values = self.qnetwork_local(state)\n",
-    "        self.qnetwork_local.train()\n",
-    "\n",
-    "        if random.random() < eps:\n",
-    "            return random.choice(np.arange(self.action_size))\n",
-    "        else:\n",
-    "            return np.argmax(action_values.cpu().data.numpy())\n",
-    "        \n",
-    "    def step(self, state, action, reward, next_state, done):\n",
-    "        state = state.reshape((1,1,state.shape[0], state.shape[1]))\n",
-    "        next_state = next_state.reshape((1,1,next_state.shape[0], next_state.shape[1]))\n",
-    "        self.memory.add(state, action, reward, next_state, done)\n",
-    "        self.t_step += 1\n",
-    "        if self.t_step % self.learn_interval == 0:\n",
-    "            if len(self.memory) > self.samples_before_learning:\n",
-    "                #state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
-    "                state = torch.from_numpy(state).float().to(device)\n",
-    "\n",
-    "                #next_state = torch.from_numpy(next_state).float().unsqueeze(0).to(device)\n",
-    "                next_state = torch.from_numpy(next_state).float().to(device)\n",
-    "\n",
-    "                target = self.qnetwork_local(state).data\n",
-    "                old_val = target[0][action]\n",
-    "                target_val = self.qnetwork_target(next_state).data\n",
-    "                if done:\n",
-    "                    target[0][action] = reward\n",
-    "                else:\n",
-    "                    target[0][action] = reward + self.gamma * torch.max(target_val)\n",
-    "                indices=None\n",
-    "                weights=None\n",
-    "                states, actions, rewards, next_states, dones = self.memory.sample()\n",
-    "\n",
-    "                self.learn(states, actions, rewards, next_states, dones, indices, weights, self.gamma)\n",
-    "        \n",
-    "    def learn(self, states, actions, rewards, next_states, dones, indices, weights, gamma):\n",
-    "        states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
-    "        actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
-    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
-    "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
-    "        dones = torch.from_numpy(np.vstack(dones)).float().to(device)\n",
-    "        states = states.reshape((BATCH_SIZE, 1, states.shape[1], states.shape[2]))\n",
-    "        next_states = next_states.reshape((BATCH_SIZE, 1, next_states.shape[1], next_states.shape[2]))\n",
-    "\n",
-    "        Q_targets_next = self.qnetwork_target(next_states).detach()\n",
-    "\n",
-    "        Q_targets_next = Q_targets_next.max(1)[0]\n",
-    "\n",
-    "        Q_targets = rewards + gamma * Q_targets_next.reshape((BATCH_SIZE, 1)) * (1 - dones)\n",
-    "\n",
-    "        pred = self.qnetwork_local(states)\n",
-    "        Q_expected = pred.gather(1, actions)\n",
-    "\n",
-    "        self.optimizer.zero_grad()\n",
-    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
-    "        loss.backward()\n",
-    "        self.optimizer.step()\n",
-    "\n",
-    "        if self.t_step % self.parameter_update_interval == 0:\n",
-    "            self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
-    "\n",
-    "    def soft_update(self, qnetwork_local, qnetwork_target, tau):\n",
-    "        for local_param, target_param in zip(qnetwork_local.parameters(), qnetwork_target.parameters()):\n",
-    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
-    "    "
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 4,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "def to_grayscale(img):\n",
-    "    return np.mean(img, axis=2).astype(np.uint8)\n",
-    "\n",
-    "def downsample(img):\n",
-    "    return img[::2, ::2]\n",
-    "\n",
-    "def preprocess(img):\n",
-    "    return to_grayscale(downsample(img))"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "h: 105 w: 80 outputs 4\n",
-      "h: 105 w: 80 outputs 4\n",
-      "Game   0 Score 1.000 completed in   175 steps eps 0.983 last 10 avg 0.000\n",
-      "Game  10 Score 1.000 completed in   166 steps eps 0.838 last 10 avg 0.000\n",
-      "Game  20 Score 1.000 completed in   160 steps eps 0.687 last 10 avg 1.273\n",
-      "Game  30 Score 3.000 completed in   238 steps eps 0.585 last 10 avg 0.727\n",
-      "Game  40 Score 2.000 completed in   197 steps eps 0.471 last 10 avg 2.091\n",
-      "Game  50 Score 2.000 completed in   211 steps eps 0.381 last 10 avg 1.727\n",
-      "Game  60 Score 3.000 completed in   245 steps eps 0.313 last 10 avg 1.455\n",
-      "Game  70 Score 0.000 completed in   179 steps eps 0.254 last 10 avg 1.455\n",
-      "Game  80 Score 0.000 completed in   153 steps eps 0.209 last 10 avg 0.727\n",
-      "Game  90 Score 1.000 completed in   185 steps eps 0.169 last 10 avg 1.545\n",
-      "Game 100 Score 2.000 completed in   231 steps eps 0.133 last 10 avg 1.636\n",
-      "Game 110 Score 1.000 completed in   251 steps eps 0.106 last 10 avg 1.545\n",
-      "Game 120 Score 4.000 completed in   384 steps eps 0.100 last 10 avg 1.455\n",
-      "Game 130 Score 0.000 completed in   180 steps eps 0.100 last 10 avg 1.636\n",
-      "Game 140 Score 1.000 completed in   232 steps eps 0.100 last 10 avg 1.455\n",
-      "Game 150 Score 2.000 completed in   270 steps eps 0.100 last 10 avg 1.364\n",
-      "Game 160 Score 0.000 completed in   185 steps eps 0.100 last 10 avg 2.091\n",
-      "Game 170 Score 1.000 completed in   176 steps eps 0.100 last 10 avg 1.909\n",
-      "Game 180 Score 5.000 completed in   421 steps eps 0.100 last 10 avg 2.091\n",
-      "Game 190 Score 3.000 completed in   228 steps eps 0.100 last 10 avg 2.909\n",
-      "Game 200 Score 3.000 completed in   274 steps eps 0.100 last 10 avg 3.000\n",
-      "Game 210 Score 3.000 completed in   464 steps eps 0.100 last 10 avg 2.909\n",
-      "Game 220 Score 3.000 completed in   253 steps eps 0.100 last 10 avg 3.091\n",
-      "Game 230 Score 3.000 completed in   239 steps eps 0.100 last 10 avg 3.455\n",
-      "Game 240 Score 3.000 completed in   246 steps eps 0.100 last 10 avg 3.727\n",
-      "Game 250 Score 7.000 completed in   255 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 260 Score 4.000 completed in   329 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 270 Score 2.000 completed in   198 steps eps 0.100 last 10 avg 3.364\n",
-      "Game 280 Score 2.000 completed in   205 steps eps 0.100 last 10 avg 3.091\n",
-      "Game 290 Score 4.000 completed in   284 steps eps 0.100 last 10 avg 3.818\n",
-      "Game 300 Score 3.000 completed in   229 steps eps 0.100 last 10 avg 3.909\n",
-      "Game 310 Score 1.000 completed in   150 steps eps 0.100 last 10 avg 3.727\n",
-      "Game 320 Score 3.000 completed in   210 steps eps 0.100 last 10 avg 3.818\n",
-      "Game 330 Score 3.000 completed in   210 steps eps 0.100 last 10 avg 3.364\n",
-      "Game 340 Score 3.000 completed in   214 steps eps 0.100 last 10 avg 5.0007\n",
-      "Game 350 Score 3.000 completed in   210 steps eps 0.100 last 10 avg 4.000\n",
-      "Game 360 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 5.3647\n",
-      "Game 370 Score 3.000 completed in   225 steps eps 0.100 last 10 avg 6.6367\n",
-      "Game 380 Score 4.000 completed in   256 steps eps 0.100 last 10 avg 4.182\n",
-      "Game 390 Score 2.000 completed in   226 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 400 Score 4.000 completed in   267 steps eps 0.100 last 10 avg 4.273\n",
-      "Game 410 Score 6.000 completed in   399 steps eps 0.100 last 10 avg 4.9091\n",
-      "Game 420 Score 5.000 completed in   308 steps eps 0.100 last 10 avg 4.545\n",
-      "Game 430 Score 4.000 completed in   239 steps eps 0.100 last 10 avg 4.4552\n",
-      "Game 440 Score 3.000 completed in   327 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 450 Score 8.000 completed in   308 steps eps 0.100 last 10 avg 5.818\n",
-      "Game 460 Score 8.000 completed in   332 steps eps 0.100 last 10 avg 5.727\n",
-      "Game 470 Score 6.000 completed in   368 steps eps 0.100 last 10 avg 5.364\n",
-      "Game 480 Score 5.000 completed in   325 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 490 Score 5.000 completed in   294 steps eps 0.100 last 10 avg 4.545\n",
-      "Game 500 Score 3.000 completed in   218 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 510 Score 4.000 completed in   268 steps eps 0.100 last 10 avg 3.455\n",
-      "Game 520 Score 1.000 completed in   168 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 530 Score 8.000 completed in   322 steps eps 0.100 last 10 avg 5.0910\n",
-      "Game 540 Score 4.000 completed in   334 steps eps 0.100 last 10 avg 4.636\n",
-      "Game 550 Score 3.000 completed in   286 steps eps 0.100 last 10 avg 4.545\n",
-      "Game 560 Score 4.000 completed in   253 steps eps 0.100 last 10 avg 3.273\n",
-      "Game 570 Score 3.000 completed in   244 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 580 Score 6.000 completed in   355 steps eps 0.100 last 10 avg 5.0918\n",
-      "Game 590 Score 8.000 completed in   326 steps eps 0.100 last 10 avg 5.0910\n",
-      "Game 600 Score 11.000 completed in   721 steps eps 0.100 last 10 avg 5.091\n",
-      "Game 610 Score 4.000 completed in   281 steps eps 0.100 last 10 avg 5.0918\n",
-      "Game 620 Score 8.000 completed in   293 steps eps 0.100 last 10 avg 5.182\n",
-      "Game 630 Score 8.000 completed in   472 steps eps 0.100 last 10 avg 5.7277\n",
-      "Game 640 Score 6.000 completed in   475 steps eps 0.100 last 10 avg 4.273\n",
-      "Game 650 Score 3.000 completed in   228 steps eps 0.100 last 10 avg 3.636\n",
-      "Game 660 Score 4.000 completed in   244 steps eps 0.100 last 10 avg 4.091\n",
-      "Game 670 Score 5.000 completed in   357 steps eps 0.100 last 10 avg 6.0919\n",
-      "Game 680 Score 4.000 completed in   287 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 690 Score 3.000 completed in   235 steps eps 0.100 last 10 avg 3.364\n",
-      "Game 700 Score 4.000 completed in   243 steps eps 0.100 last 10 avg 4.000\n",
-      "Game 710 Score 6.000 completed in   404 steps eps 0.100 last 10 avg 4.818\n",
-      "Game 720 Score 5.000 completed in   311 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 730 Score 5.000 completed in   333 steps eps 0.100 last 10 avg 4.273\n",
-      "Game 740 Score 5.000 completed in   325 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 750 Score 2.000 completed in   181 steps eps 0.100 last 10 avg 4.273\n",
-      "Game 760 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 4.3649\n",
-      "Game 770 Score 11.000 completed in   445 steps eps 0.100 last 10 avg 5.636\n",
-      "Game 780 Score 4.000 completed in   243 steps eps 0.100 last 10 avg 5.545\n",
-      "Game 790 Score 3.000 completed in   246 steps eps 0.100 last 10 avg 4.6365\n",
-      "Game 800 Score 4.000 completed in   318 steps eps 0.100 last 10 avg 4.636\n",
-      "Game 810 Score 4.000 completed in   275 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 820 Score 3.000 completed in   224 steps eps 0.100 last 10 avg 4.9090\n",
-      "Game 830 Score 3.000 completed in   209 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 840 Score 3.000 completed in   221 steps eps 0.100 last 10 avg 5.636\n",
-      "Game 850 Score 5.000 completed in   288 steps eps 0.100 last 10 avg 4.909\n",
-      "Game 860 Score 7.000 completed in   250 steps eps 0.100 last 10 avg 4.182\n",
-      "Game 870 Score 6.000 completed in   340 steps eps 0.100 last 10 avg 4.818\n",
-      "Game 880 Score 6.000 completed in   339 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 890 Score 5.000 completed in   315 steps eps 0.100 last 10 avg 4.636\n",
-      "Game 900 Score 10.000 completed in   434 steps eps 0.100 last 10 avg 4.818\n",
-      "Game 910 Score 5.000 completed in   289 steps eps 0.100 last 10 avg 4.636\n",
-      "Game 920 Score 7.000 completed in   270 steps eps 0.100 last 10 avg 4.818\n",
-      "Game 930 Score 5.000 completed in   344 steps eps 0.100 last 10 avg 4.727\n",
-      "Game 940 Score 8.000 completed in   481 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 950 Score 4.000 completed in   258 steps eps 0.100 last 10 avg 4.909\n",
-      "Game 960 Score 3.000 completed in   246 steps eps 0.100 last 10 avg 5.3647\n",
-      "Game 970 Score 5.000 completed in   272 steps eps 0.100 last 10 avg 5.4558\n",
-      "Game 980 Score 6.000 completed in   300 steps eps 0.100 last 10 avg 4.182\n",
-      "Game 990 Score 5.000 completed in   320 steps eps 0.100 last 10 avg 4.273\n",
-      "Game 1000 Score 3.000 completed in   227 steps eps 0.100 last 10 avg 5.636\n",
-      "Game 1010 Score 9.000 completed in   334 steps eps 0.100 last 10 avg 5.6361\n",
-      "Game 1020 Score 5.000 completed in   289 steps eps 0.100 last 10 avg 7.0916\n",
-      "Game 1030 Score 7.000 completed in   351 steps eps 0.100 last 10 avg 5.455\n",
-      "Game 1040 Score 3.000 completed in   229 steps eps 0.100 last 10 avg 5.5454\n",
-      "Game 1050 Score 4.000 completed in   281 steps eps 0.100 last 10 avg 4.000\n",
-      "Game 1060 Score 4.000 completed in   434 steps eps 0.100 last 10 avg 4.727\n",
-      "Game 1070 Score 5.000 completed in   304 steps eps 0.100 last 10 avg 5.273\n",
-      "Game 1080 Score 10.000 completed in   496 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 1090 Score 3.000 completed in   228 steps eps 0.100 last 10 avg 4.636\n",
-      "Game 1100 Score 4.000 completed in   242 steps eps 0.100 last 10 avg 5.182\n",
-      "Game 1110 Score 8.000 completed in   448 steps eps 0.100 last 10 avg 6.0008\n",
-      "Game 1120 Score 8.000 completed in   470 steps eps 0.100 last 10 avg 6.1821\n",
-      "Game 1130 Score 5.000 completed in   288 steps eps 0.100 last 10 avg 6.3646\n",
-      "Game 1140 Score 6.000 completed in   373 steps eps 0.100 last 10 avg 4.182\n",
-      "Game 1150 Score 8.000 completed in   291 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 1160 Score 2.000 completed in   179 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 1170 Score 5.000 completed in   312 steps eps 0.100 last 10 avg 5.0001\n",
-      "Game 1180 Score 4.000 completed in   256 steps eps 0.100 last 10 avg 4.9095\n",
-      "Game 1190 Score 7.000 completed in   474 steps eps 0.100 last 10 avg 5.0910\n",
-      "Game 1200 Score 3.000 completed in   209 steps eps 0.100 last 10 avg 5.3644\n",
-      "Game 1210 Score 3.000 completed in   213 steps eps 0.100 last 10 avg 6.3648\n",
-      "Game 1220 Score 1.000 completed in   151 steps eps 0.100 last 10 avg 5.0918\n",
-      "Game 1230 Score 6.000 completed in   435 steps eps 0.100 last 10 avg 5.636\n",
-      "Game 1240 Score 3.000 completed in   228 steps eps 0.100 last 10 avg 5.7278\n",
-      "Game 1250 Score 9.000 completed in   457 steps eps 0.100 last 10 avg 4.182\n",
-      "Game 1260 Score 7.000 completed in   250 steps eps 0.100 last 10 avg 5.8184\n",
-      "Game 1270 Score 2.000 completed in   199 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 1280 Score 6.000 completed in   336 steps eps 0.100 last 10 avg 3.545\n",
-      "Game 1290 Score 5.000 completed in   307 steps eps 0.100 last 10 avg 5.455\n",
-      "Game 1300 Score 4.000 completed in   272 steps eps 0.100 last 10 avg 4.727\n",
-      "Game 1310 Score 5.000 completed in   287 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 1320 Score 5.000 completed in   287 steps eps 0.100 last 10 avg 6.3645\n",
-      "Game 1330 Score 3.000 completed in   246 steps eps 0.100 last 10 avg 5.727\n",
-      "Game 1340 Score 3.000 completed in   227 steps eps 0.100 last 10 avg 5.364\n",
-      "Game 1350 Score 3.000 completed in   225 steps eps 0.100 last 10 avg 4.727\n",
-      "Game 1360 Score 4.000 completed in   236 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 1370 Score 4.000 completed in   261 steps eps 0.100 last 10 avg 4.636\n",
-      "Game 1380 Score 7.000 completed in   369 steps eps 0.100 last 10 avg 5.455\n",
-      "Game 1390 Score 13.000 completed in   418 steps eps 0.100 last 10 avg 6.182\n",
-      "Game 1400 Score 8.000 completed in   458 steps eps 0.100 last 10 avg 7.1828\n",
-      "Game 1410 Score 7.000 completed in   409 steps eps 0.100 last 10 avg 6.000\n",
-      "Game 1420 Score 5.000 completed in   325 steps eps 0.100 last 10 avg 5.273\n",
-      "Game 1430 Score 11.000 completed in   442 steps eps 0.100 last 10 avg 6.909\n",
-      "Game 1440 Score 5.000 completed in   272 steps eps 0.100 last 10 avg 7.0918\n",
-      "Game 1450 Score 4.000 completed in   263 steps eps 0.100 last 10 avg 5.7279\n",
-      "Game 1460 Score 8.000 completed in   414 steps eps 0.100 last 10 avg 4.909\n",
-      "Game 1470 Score 5.000 completed in   275 steps eps 0.100 last 10 avg 5.9096\n",
-      "Game 1480 Score 4.000 completed in   257 steps eps 0.100 last 10 avg 5.0914\n",
-      "Game 1490 Score 5.000 completed in   377 steps eps 0.100 last 10 avg 6.5456\n",
-      "Game 1500 Score 8.000 completed in   424 steps eps 0.100 last 10 avg 5.8188\n",
-      "Game 1510 Score 5.000 completed in   296 steps eps 0.100 last 10 avg 5.8183\n",
-      "Game 1520 Score 6.000 completed in   329 steps eps 0.100 last 10 avg 4.9095\n",
-      "Game 1530 Score 5.000 completed in   289 steps eps 0.100 last 10 avg 6.0914\n",
-      "Game 1540 Score 6.000 completed in   358 steps eps 0.100 last 10 avg 5.545\n",
-      "Game 1550 Score 2.000 completed in   183 steps eps 0.100 last 10 avg 4.909\n",
-      "Game 1560 Score 4.000 completed in   241 steps eps 0.100 last 10 avg 5.455\n",
-      "Game 1570 Score 6.000 completed in   377 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 1580 Score 6.000 completed in   339 steps eps 0.100 last 10 avg 5.1820\n",
-      "Game 1590 Score 7.000 completed in   372 steps eps 0.100 last 10 avg 5.091\n",
-      "Game 1600 Score 5.000 completed in   287 steps eps 0.100 last 10 avg 6.0915\n",
-      "Game 1610 Score 7.000 completed in   358 steps eps 0.100 last 10 avg 5.273\n",
-      "Game 1620 Score 5.000 completed in   323 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 1630 Score 5.000 completed in   310 steps eps 0.100 last 10 avg 6.091\n",
-      "Game 1640 Score 7.000 completed in   429 steps eps 0.100 last 10 avg 6.8189\n",
-      "Game 1650 Score 8.000 completed in   286 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 1660 Score 4.000 completed in   268 steps eps 0.100 last 10 avg 6.3648\n",
-      "Game 1670 Score 4.000 completed in   295 steps eps 0.100 last 10 avg 5.7277\n",
-      "Game 1680 Score 5.000 completed in   307 steps eps 0.100 last 10 avg 4.4553\n",
-      "Game 1690 Score 4.000 completed in   261 steps eps 0.100 last 10 avg 5.9095\n",
-      "Game 1700 Score 6.000 completed in   340 steps eps 0.100 last 10 avg 6.1827\n",
-      "Game 1710 Score 9.000 completed in   326 steps eps 0.100 last 10 avg 7.2737\n",
-      "Game 1720 Score 4.000 completed in   242 steps eps 0.100 last 10 avg 5.455\n",
-      "Game 1730 Score 6.000 completed in   337 steps eps 0.100 last 10 avg 5.0001\n",
-      "Game 1740 Score 4.000 completed in   300 steps eps 0.100 last 10 avg 5.727\n",
-      "Game 1750 Score 9.000 completed in   411 steps eps 0.100 last 10 avg 6.3642\n",
-      "Game 1760 Score 8.000 completed in   418 steps eps 0.100 last 10 avg 6.091\n",
-      "Game 1770 Score 9.000 completed in   436 steps eps 0.100 last 10 avg 6.455\n",
-      "Game 1780 Score 6.000 completed in   381 steps eps 0.100 last 10 avg 5.818\n",
-      "Game 1790 Score 7.000 completed in   367 steps eps 0.100 last 10 avg 5.091\n",
-      "Game 1800 Score 10.000 completed in   485 steps eps 0.100 last 10 avg 6.727\n",
-      "Game 1810 Score 6.000 completed in   397 steps eps 0.100 last 10 avg 5.545\n",
-      "Game 1820 Score 6.000 completed in   377 steps eps 0.100 last 10 avg 6.2739\n",
-      "Game 1830 Score 8.000 completed in   427 steps eps 0.100 last 10 avg 5.727\n",
-      "Game 1840 Score 5.000 completed in   285 steps eps 0.100 last 10 avg 7.1826\n",
-      "Game 1850 Score 3.000 completed in   225 steps eps 0.100 last 10 avg 5.4551\n",
-      "Game 1860 Score 5.000 completed in   285 steps eps 0.100 last 10 avg 5.091\n",
-      "Game 1870 Score 6.000 completed in   379 steps eps 0.100 last 10 avg 5.364\n",
-      "Game 1880 Score 5.000 completed in   290 steps eps 0.100 last 10 avg 6.0003\n",
-      "Game 1890 Score 6.000 completed in   306 steps eps 0.100 last 10 avg 4.545\n",
-      "Game 1900 Score 5.000 completed in   291 steps eps 0.100 last 10 avg 6.000\n",
-      "Game 1910 Score 5.000 completed in   288 steps eps 0.100 last 10 avg 5.182\n",
-      "Game 1920 Score 3.000 completed in   229 steps eps 0.100 last 10 avg 4.818\n",
-      "Game 1930 Score 5.000 completed in   290 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 1940 Score 8.000 completed in   437 steps eps 0.100 last 10 avg 6.5457\n",
-      "Game 1950 Score 4.000 completed in   244 steps eps 0.100 last 10 avg 6.545\n",
-      "Game 1960 Score 4.000 completed in   303 steps eps 0.100 last 10 avg 6.0916\n",
-      "Game 1970 Score 5.000 completed in   308 steps eps 0.100 last 10 avg 4.818\n",
-      "Game 1980 Score 3.000 completed in   210 steps eps 0.100 last 10 avg 4.273\n",
-      "Game 1990 Score 4.000 completed in   263 steps eps 0.100 last 10 avg 4.8185\n",
-      "Game 2000 Score 6.000 completed in   374 steps eps 0.100 last 10 avg 5.636\n",
-      "Game 2010 Score 7.000 completed in   247 steps eps 0.100 last 10 avg 4.8188\n",
-      "Game 2020 Score 7.000 completed in   268 steps eps 0.100 last 10 avg 6.545\n",
-      "Game 2030 Score 6.000 completed in   317 steps eps 0.100 last 10 avg 4.909\n",
-      "Game 2040 Score 10.000 completed in   448 steps eps 0.100 last 10 avg 5.727\n",
-      "Game 2050 Score 8.000 completed in   439 steps eps 0.100 last 10 avg 5.909\n",
-      "Game 2060 Score 5.000 completed in   290 steps eps 0.100 last 10 avg 6.0007\n",
-      "Game 2070 Score 3.000 completed in   230 steps eps 0.100 last 10 avg 4.545\n",
-      "Game 2080 Score 4.000 completed in   241 steps eps 0.100 last 10 avg 4.000\n",
-      "Game 2090 Score 9.000 completed in   326 steps eps 0.100 last 10 avg 6.0007\n",
-      "Game 2100 Score 2.000 completed in   263 steps eps 0.100 last 10 avg 5.273\n",
-      "Game 2110 Score 5.000 completed in   346 steps eps 0.100 last 10 avg 4.545\n",
-      "Game 2120 Score 6.000 completed in   321 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 2130 Score 6.000 completed in   328 steps eps 0.100 last 10 avg 6.8183\n",
-      "Game 2140 Score 4.000 completed in   258 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 2150 Score 5.000 completed in   327 steps eps 0.100 last 10 avg 4.636\n",
-      "Game 2160 Score 3.000 completed in   211 steps eps 0.100 last 10 avg 5.9091\n"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Game 2170 Score 5.000 completed in   305 steps eps 0.100 last 10 avg 4.727\n",
-      "Game 2180 Score 10.000 completed in   445 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 2190 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 5.636\n",
-      "Game 2200 Score 10.000 completed in   402 steps eps 0.100 last 10 avg 5.545\n",
-      "Game 2210 Score 5.000 completed in   320 steps eps 0.100 last 10 avg 5.273\n",
-      "Game 2220 Score 3.000 completed in   209 steps eps 0.100 last 10 avg 4.000\n",
-      "Game 2230 Score 7.000 completed in   492 steps eps 0.100 last 10 avg 4.818\n",
-      "Game 2240 Score 4.000 completed in   241 steps eps 0.100 last 10 avg 5.364\n",
-      "Game 2250 Score 5.000 completed in   307 steps eps 0.100 last 10 avg 5.1823\n",
-      "Game 2260 Score 4.000 completed in   241 steps eps 0.100 last 10 avg 6.0914\n",
-      "Game 2270 Score 8.000 completed in   321 steps eps 0.100 last 10 avg 5.8185\n",
-      "Game 2280 Score 8.000 completed in   396 steps eps 0.100 last 10 avg 6.091\n",
-      "Game 2290 Score 5.000 completed in   350 steps eps 0.100 last 10 avg 6.3644\n",
-      "Game 2300 Score 4.000 completed in   254 steps eps 0.100 last 10 avg 4.545\n",
-      "Game 2310 Score 4.000 completed in   243 steps eps 0.100 last 10 avg 5.4556\n",
-      "Game 2320 Score 10.000 completed in   463 steps eps 0.100 last 10 avg 5.182\n",
-      "Game 2330 Score 6.000 completed in   321 steps eps 0.100 last 10 avg 6.273\n",
-      "Game 2340 Score 3.000 completed in   208 steps eps 0.100 last 10 avg 5.818\n",
-      "Game 2350 Score 11.000 completed in   431 steps eps 0.100 last 10 avg 5.545\n",
-      "Game 2360 Score 1.000 completed in   150 steps eps 0.100 last 10 avg 5.818\n",
-      "Game 2370 Score 7.000 completed in   344 steps eps 0.100 last 10 avg 6.3643\n",
-      "Game 2380 Score 8.000 completed in   405 steps eps 0.100 last 10 avg 6.273\n",
-      "Game 2390 Score 5.000 completed in   370 steps eps 0.100 last 10 avg 6.273\n",
-      "Game 2398 Score 8.000 completed in   421 steps eps 0.100 last 10 avg 7.1829"
-     ]
-    }
-   ],
-   "source": [
-    "\n",
-    "\n",
-    "\n",
-    "env = gym.make('BreakoutDeterministic-v4')\n",
-    "action_size = env.action_space.n\n",
-    "state = env.reset()\n",
-    "height, width = preprocess(state).shape\n",
-    "EPS_DECAY  = 0.9999\n",
-    "EPS_MIN = 0.1\n",
-    "\n",
-    "# if gpu is to be used\n",
-    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
-    "\n",
-    "num_episodes = 2500\n",
-    "agent = DQNAgent(height, width, action_size, 0)\n",
-    "\n",
-    "scores = []\n",
-    "eps = 1.0\n",
-    "for i_episode in range(num_episodes):\n",
-    "    # Initialize the environment and state\n",
-    "    state = env.reset()\n",
-    "    state = preprocess(state)\n",
-    "    score = 0\n",
-    "    steps = 0\n",
-    "    while True:\n",
-    "        # Select and perform an action\n",
-    "        action = agent.act(state, eps)\n",
-    "        next_state, reward, done, _ = env.step(action.item())\n",
-    "        next_state = preprocess(next_state)\n",
-    "        reward = torch.tensor([reward], device=device)\n",
-    "\n",
-    "        agent.step(state, action, reward, next_state, done)\n",
-    "        \n",
-    "        score += reward.cpu()[0]\n",
-    "        # Move to the next state\n",
-    "        state = next_state\n",
-    "        eps *= EPS_DECAY\n",
-    "        eps = max(eps, EPS_MIN)\n",
-    "\n",
-    "        steps += 1\n",
-    "        if done:\n",
-    "            break\n",
-    "            \n",
-    "    scores.append(score)\n",
-    "    last10_mean = 0.\n",
-    "    if i_episode > 10:\n",
-    "        last10_mean = np.mean(scores[i_episode-10:])\n",
-    "    print(\"\\rGame {:3d} Score {:.3f} completed in {:5d} steps eps {:.3f} last 10 avg {:.3f}\".format(\n",
-    "        i_episode, score, steps, eps, last10_mean), end=\"\")\n",
-    "    if i_episode % 10 == 0:\n",
-    "        print()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import torch\n",
-    "v=torch.LongTensor([1])\n",
-    "print(\"{:3d}\".format(v[0]))"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": "Python 3",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.6.8"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 2
-}
diff --git a/DRLIntro.ipynb b/DRLIntro.ipynb
index 98acc73..0fd113a 100644
--- a/DRLIntro.ipynb
+++ b/DRLIntro.ipynb
@@ -906,6 +906,7 @@
     "import matplotlib.pyplot as plt\n",
     "from collections import namedtuple\n",
     "from itertools import count\n",
+    "from PIL import Image\n",
     "\n",
     "import torch\n",
     "import torch.nn as nn\n",
@@ -962,28 +963,29 @@
     "        super(DQN, self).__init__()\n",
     "        print(\"h: {} w: {} outputs {}\".format(h, w, outputs))\n",
     "        self.seed = torch.manual_seed(seed)\n",
-    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=8, stride=4)\n",
-    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)\n",
+    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2)\n",
+    "        self.bn1 = nn.BatchNorm2d(16)\n",
+    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
+    "        self.bn2 = nn.BatchNorm2d(32)\n",
+    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
+    "        self.bn3 = nn.BatchNorm2d(32)\n",
     "\n",
     "        # Number of Linear input connections depends on output of conv2d layers\n",
     "        # and therefore the input image size, so compute it.\n",
-    "        def conv2d_size_out(size, kernel_size, stride):\n",
+    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
     "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
-    "        convw = conv2d_size_out(conv2d_size_out(w,kernel_size=8,stride=4), kernel_size=4, stride=2)\n",
-    "        convh = conv2d_size_out(conv2d_size_out(h, kernel_size=8, stride=4), \n",
-    "                                                kernel_size=4, stride=2)\n",
+    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
+    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
     "        linear_input_size = convw * convh * 32\n",
-    "        self.fc1 = nn.Linear(linear_input_size, 512)\n",
-    "        self.fc2 = nn.Linear(512, outputs)\n",
-    "        \n",
+    "        self.head = nn.Linear(linear_input_size, outputs)\n",
     "\n",
     "    # Called with either one element to determine next action, or a batch\n",
     "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
     "    def forward(self, x):\n",
-    "        x = F.relu(self.conv1(x))\n",
-    "        x = F.relu(self.conv2(x))\n",
-    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
-    "        return self.fc2(x)"
+    "        x = F.relu(self.bn1(self.conv1(x)))\n",
+    "        x = F.relu(self.bn2(self.conv2(x)))\n",
+    "        x = F.relu(self.bn3(self.conv3(x)))\n",
+    "        return self.head(x.view(x.size(0), -1))"
    ]
   },
   {
@@ -997,15 +999,15 @@
     "BUFFER_SIZE = int(1e5)\n",
     "TAU = 0.01\n",
     "GAMMA = 0.99\n",
-    "LR=5e-4\n",
+    "LR=1e-3\n",
     "class DQNAgent:\n",
     "    def __init__(self, height, width, action_size, seed):\n",
     "        self.seed = random.seed(seed)\n",
     "        self.action_size = action_size\n",
     "        self.batch_indices = torch.arange(BATCH_SIZE).long().to(device)\n",
     "        self.samples_before_learning = 1000\n",
-    "        self.learn_interval = 20\n",
-    "        self.parameter_update_interval = 2\n",
+    "        self.learn_interval = 10\n",
+    "        self.parameter_update_interval = 4\n",
     "        self.tau = TAU\n",
     "        self.gamma = GAMMA\n",
     "\n",
@@ -1107,7 +1109,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": null,
    "metadata": {},
    "outputs": [
     {
@@ -1116,269 +1118,259 @@
      "text": [
       "h: 105 w: 80 outputs 4\n",
       "h: 105 w: 80 outputs 4\n",
-      "Game   0 Score 1.000 completed in   175 steps eps 0.991 last 10 avg 0.000\n",
-      "Game  10 Score 0.000 completed in   129 steps eps 0.897 last 10 avg 0.000\n",
-      "Game  20 Score 0.000 completed in   131 steps eps 0.817 last 10 avg 1.091\n",
-      "Game  30 Score 1.000 completed in   158 steps eps 0.749 last 10 avg 1.000\n",
-      "Game  40 Score 0.000 completed in   140 steps eps 0.685 last 10 avg 1.000\n",
-      "Game  50 Score 7.000 completed in   401 steps eps 0.615 last 10 avg 1.909\n",
-      "Game  60 Score 0.000 completed in   154 steps eps 0.563 last 10 avg 1.455\n",
-      "Game  70 Score 0.000 completed in   124 steps eps 0.518 last 10 avg 0.727\n",
-      "Game  80 Score 0.000 completed in   153 steps eps 0.480 last 10 avg 0.545\n",
-      "Game  90 Score 0.000 completed in   145 steps eps 0.443 last 10 avg 0.455\n",
-      "Game 100 Score 1.000 completed in   225 steps eps 0.404 last 10 avg 1.182\n",
-      "Game 110 Score 1.000 completed in   195 steps eps 0.371 last 10 avg 0.909\n",
-      "Game 120 Score 2.000 completed in   201 steps eps 0.340 last 10 avg 1.091\n",
-      "Game 130 Score 3.000 completed in   232 steps eps 0.310 last 10 avg 1.545\n",
-      "Game 140 Score 2.000 completed in   223 steps eps 0.282 last 10 avg 1.636\n",
-      "Game 150 Score 1.000 completed in   173 steps eps 0.256 last 10 avg 1.455\n",
-      "Game 160 Score 1.000 completed in   176 steps eps 0.234 last 10 avg 1.273\n",
-      "Game 170 Score 3.000 completed in   213 steps eps 0.213 last 10 avg 1.455\n",
-      "Game 180 Score 0.000 completed in   122 steps eps 0.196 last 10 avg 1.364\n",
-      "Game 190 Score 2.000 completed in   244 steps eps 0.177 last 10 avg 1.545\n",
-      "Game 200 Score 0.000 completed in   123 steps eps 0.164 last 10 avg 1.364\n",
-      "Game 210 Score 2.000 completed in   205 steps eps 0.150 last 10 avg 1.182\n",
-      "Game 220 Score 2.000 completed in   275 steps eps 0.135 last 10 avg 1.636\n",
-      "Game 230 Score 1.000 completed in   179 steps eps 0.123 last 10 avg 1.636\n",
-      "Game 240 Score 0.000 completed in   139 steps eps 0.113 last 10 avg 1.182\n",
-      "Game 250 Score 0.000 completed in   189 steps eps 0.100 last 10 avg 1.909\n",
-      "Game 260 Score 1.000 completed in   191 steps eps 0.100 last 10 avg 1.545\n",
-      "Game 270 Score 5.000 completed in   418 steps eps 0.100 last 10 avg 2.727\n",
-      "Game 280 Score 3.000 completed in   210 steps eps 0.100 last 10 avg 1.636\n",
-      "Game 290 Score 2.000 completed in   200 steps eps 0.100 last 10 avg 3.273\n",
-      "Game 300 Score 3.000 completed in   232 steps eps 0.100 last 10 avg 3.0914\n",
-      "Game 310 Score 2.000 completed in   194 steps eps 0.100 last 10 avg 2.455\n",
-      "Game 320 Score 3.000 completed in   247 steps eps 0.100 last 10 avg 2.364\n",
-      "Game 330 Score 4.000 completed in   309 steps eps 0.100 last 10 avg 2.000\n",
-      "Game 340 Score 4.000 completed in   290 steps eps 0.100 last 10 avg 2.545\n",
-      "Game 350 Score 3.000 completed in   269 steps eps 0.100 last 10 avg 2.909\n",
-      "Game 360 Score 5.000 completed in   311 steps eps 0.100 last 10 avg 2.818\n",
-      "Game 370 Score 3.000 completed in   213 steps eps 0.100 last 10 avg 3.000\n",
-      "Game 380 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 2.545\n",
-      "Game 390 Score 3.000 completed in   224 steps eps 0.100 last 10 avg 2.909\n",
-      "Game 400 Score 2.000 completed in   198 steps eps 0.100 last 10 avg 3.182\n",
-      "Game 410 Score 2.000 completed in   194 steps eps 0.100 last 10 avg 3.636\n",
-      "Game 420 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 3.545\n",
-      "Game 430 Score 1.000 completed in   170 steps eps 0.100 last 10 avg 2.545\n",
-      "Game 440 Score 3.000 completed in   235 steps eps 0.100 last 10 avg 2.818\n",
-      "Game 450 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 3.273\n",
-      "Game 460 Score 1.000 completed in   150 steps eps 0.100 last 10 avg 3.000\n",
-      "Game 470 Score 3.000 completed in   213 steps eps 0.100 last 10 avg 3.091\n",
-      "Game 480 Score 3.000 completed in   235 steps eps 0.100 last 10 avg 3.182\n",
-      "Game 490 Score 3.000 completed in   213 steps eps 0.100 last 10 avg 2.545\n",
-      "Game 500 Score 3.000 completed in   303 steps eps 0.100 last 10 avg 3.727\n",
-      "Game 510 Score 3.000 completed in   213 steps eps 0.100 last 10 avg 3.273\n",
-      "Game 520 Score 4.000 completed in   289 steps eps 0.100 last 10 avg 3.455\n",
-      "Game 530 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 3.000\n",
-      "Game 540 Score 4.000 completed in   258 steps eps 0.100 last 10 avg 3.636\n",
-      "Game 550 Score 1.000 completed in   161 steps eps 0.100 last 10 avg 3.091\n",
-      "Game 560 Score 3.000 completed in   248 steps eps 0.100 last 10 avg 3.545\n",
-      "Game 570 Score 3.000 completed in   223 steps eps 0.100 last 10 avg 3.636\n",
-      "Game 580 Score 4.000 completed in   243 steps eps 0.100 last 10 avg 3.909\n",
-      "Game 590 Score 2.000 completed in   179 steps eps 0.100 last 10 avg 3.636\n",
-      "Game 600 Score 3.000 completed in   218 steps eps 0.100 last 10 avg 3.636\n",
-      "Game 610 Score 7.000 completed in   404 steps eps 0.100 last 10 avg 3.182\n",
-      "Game 620 Score 2.000 completed in   179 steps eps 0.100 last 10 avg 3.909\n",
-      "Game 630 Score 5.000 completed in   271 steps eps 0.100 last 10 avg 3.545\n",
-      "Game 640 Score 6.000 completed in   328 steps eps 0.100 last 10 avg 4.182\n",
-      "Game 650 Score 5.000 completed in   286 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 660 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 4.5455\n",
-      "Game 670 Score 2.000 completed in   231 steps eps 0.100 last 10 avg 3.9092\n",
-      "Game 680 Score 2.000 completed in   236 steps eps 0.100 last 10 avg 4.7273\n",
-      "Game 690 Score 2.000 completed in   186 steps eps 0.100 last 10 avg 3.727\n",
-      "Game 700 Score 3.000 completed in   317 steps eps 0.100 last 10 avg 3.636\n",
-      "Game 710 Score 4.000 completed in   256 steps eps 0.100 last 10 avg 4.091\n",
-      "Game 720 Score 5.000 completed in   357 steps eps 0.100 last 10 avg 4.727\n",
-      "Game 730 Score 5.000 completed in   290 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 740 Score 4.000 completed in   259 steps eps 0.100 last 10 avg 4.3643\n",
-      "Game 750 Score 6.000 completed in   315 steps eps 0.100 last 10 avg 5.091\n",
-      "Game 760 Score 4.000 completed in   301 steps eps 0.100 last 10 avg 5.7275\n",
-      "Game 770 Score 2.000 completed in   203 steps eps 0.100 last 10 avg 5.273\n",
-      "Game 780 Score 5.000 completed in   313 steps eps 0.100 last 10 avg 3.545\n",
-      "Game 790 Score 6.000 completed in   348 steps eps 0.100 last 10 avg 4.727\n",
-      "Game 800 Score 4.000 completed in   283 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 810 Score 5.000 completed in   307 steps eps 0.100 last 10 avg 4.545\n",
-      "Game 820 Score 5.000 completed in   293 steps eps 0.100 last 10 avg 5.9098\n",
-      "Game 830 Score 2.000 completed in   182 steps eps 0.100 last 10 avg 4.818\n",
-      "Game 840 Score 2.000 completed in   199 steps eps 0.100 last 10 avg 3.818\n",
-      "Game 850 Score 2.000 completed in   293 steps eps 0.100 last 10 avg 4.7279\n",
-      "Game 860 Score 4.000 completed in   256 steps eps 0.100 last 10 avg 4.182\n",
-      "Game 870 Score 4.000 completed in   331 steps eps 0.100 last 10 avg 5.0009\n",
-      "Game 880 Score 6.000 completed in   428 steps eps 0.100 last 10 avg 4.636\n",
-      "Game 890 Score 8.000 completed in   305 steps eps 0.100 last 10 avg 5.364\n",
-      "Game 900 Score 9.000 completed in   421 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 910 Score 4.000 completed in   274 steps eps 0.100 last 10 avg 5.182\n",
-      "Game 920 Score 6.000 completed in   334 steps eps 0.100 last 10 avg 4.000\n",
-      "Game 930 Score 6.000 completed in   417 steps eps 0.100 last 10 avg 4.909\n",
-      "Game 940 Score 2.000 completed in   220 steps eps 0.100 last 10 avg 4.182\n",
-      "Game 950 Score 6.000 completed in   341 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 960 Score 11.000 completed in   430 steps eps 0.100 last 10 avg 6.364\n",
-      "Game 970 Score 6.000 completed in   348 steps eps 0.100 last 10 avg 6.182\n",
-      "Game 980 Score 7.000 completed in   374 steps eps 0.100 last 10 avg 6.1825\n",
-      "Game 990 Score 6.000 completed in   365 steps eps 0.100 last 10 avg 5.364\n",
-      "Game 1000 Score 4.000 completed in   315 steps eps 0.100 last 10 avg 4.818\n",
-      "Game 1010 Score 5.000 completed in   286 steps eps 0.100 last 10 avg 6.3648\n",
-      "Game 1020 Score 6.000 completed in   361 steps eps 0.100 last 10 avg 4.818\n",
-      "Game 1030 Score 6.000 completed in   476 steps eps 0.100 last 10 avg 5.182\n",
-      "Game 1040 Score 6.000 completed in   343 steps eps 0.100 last 10 avg 4.818\n",
-      "Game 1050 Score 5.000 completed in   302 steps eps 0.100 last 10 avg 4.7278\n",
-      "Game 1060 Score 4.000 completed in   296 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 1070 Score 2.000 completed in   316 steps eps 0.100 last 10 avg 5.364\n",
-      "Game 1080 Score 4.000 completed in   275 steps eps 0.100 last 10 avg 5.455\n"
+      "Game   0 Score 1.000 completed in   175 steps eps 0.983 last 10 avg 0.000\n",
+      "Game  10 Score 1.000 completed in   166 steps eps 0.838 last 10 avg 0.000\n",
+      "Game  20 Score 1.000 completed in   160 steps eps 0.687 last 10 avg 1.273\n",
+      "Game  30 Score 3.000 completed in   238 steps eps 0.585 last 10 avg 0.727\n",
+      "Game  40 Score 2.000 completed in   197 steps eps 0.471 last 10 avg 2.091\n",
+      "Game  50 Score 2.000 completed in   211 steps eps 0.381 last 10 avg 1.727\n",
+      "Game  60 Score 3.000 completed in   245 steps eps 0.313 last 10 avg 1.455\n",
+      "Game  70 Score 0.000 completed in   179 steps eps 0.254 last 10 avg 1.455\n",
+      "Game  80 Score 0.000 completed in   153 steps eps 0.209 last 10 avg 0.727\n",
+      "Game  90 Score 1.000 completed in   185 steps eps 0.169 last 10 avg 1.545\n",
+      "Game 100 Score 2.000 completed in   231 steps eps 0.133 last 10 avg 1.636\n",
+      "Game 110 Score 1.000 completed in   251 steps eps 0.106 last 10 avg 1.545\n",
+      "Game 120 Score 4.000 completed in   384 steps eps 0.100 last 10 avg 1.455\n",
+      "Game 130 Score 0.000 completed in   180 steps eps 0.100 last 10 avg 1.636\n",
+      "Game 140 Score 1.000 completed in   232 steps eps 0.100 last 10 avg 1.455\n",
+      "Game 150 Score 2.000 completed in   270 steps eps 0.100 last 10 avg 1.364\n",
+      "Game 160 Score 0.000 completed in   185 steps eps 0.100 last 10 avg 2.091\n",
+      "Game 170 Score 1.000 completed in   176 steps eps 0.100 last 10 avg 1.909\n",
+      "Game 180 Score 5.000 completed in   421 steps eps 0.100 last 10 avg 2.091\n",
+      "Game 190 Score 3.000 completed in   228 steps eps 0.100 last 10 avg 2.909\n",
+      "Game 200 Score 3.000 completed in   274 steps eps 0.100 last 10 avg 3.000\n",
+      "Game 210 Score 3.000 completed in   464 steps eps 0.100 last 10 avg 2.909\n",
+      "Game 220 Score 3.000 completed in   253 steps eps 0.100 last 10 avg 3.091\n",
+      "Game 230 Score 3.000 completed in   239 steps eps 0.100 last 10 avg 3.455\n",
+      "Game 240 Score 3.000 completed in   246 steps eps 0.100 last 10 avg 3.727\n",
+      "Game 250 Score 7.000 completed in   255 steps eps 0.100 last 10 avg 4.455\n",
+      "Game 260 Score 4.000 completed in   329 steps eps 0.100 last 10 avg 4.364\n",
+      "Game 270 Score 2.000 completed in   198 steps eps 0.100 last 10 avg 3.364\n",
+      "Game 280 Score 2.000 completed in   205 steps eps 0.100 last 10 avg 3.091\n",
+      "Game 290 Score 4.000 completed in   284 steps eps 0.100 last 10 avg 3.818\n",
+      "Game 300 Score 3.000 completed in   229 steps eps 0.100 last 10 avg 3.909\n",
+      "Game 310 Score 1.000 completed in   150 steps eps 0.100 last 10 avg 3.727\n",
+      "Game 320 Score 3.000 completed in   210 steps eps 0.100 last 10 avg 3.818\n",
+      "Game 330 Score 3.000 completed in   210 steps eps 0.100 last 10 avg 3.364\n",
+      "Game 340 Score 3.000 completed in   214 steps eps 0.100 last 10 avg 5.0007\n",
+      "Game 350 Score 3.000 completed in   210 steps eps 0.100 last 10 avg 4.000\n",
+      "Game 360 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 5.3647\n",
+      "Game 370 Score 3.000 completed in   225 steps eps 0.100 last 10 avg 6.6367\n",
+      "Game 380 Score 4.000 completed in   256 steps eps 0.100 last 10 avg 4.182\n",
+      "Game 390 Score 2.000 completed in   226 steps eps 0.100 last 10 avg 5.000\n",
+      "Game 400 Score 4.000 completed in   267 steps eps 0.100 last 10 avg 4.273\n",
+      "Game 410 Score 6.000 completed in   399 steps eps 0.100 last 10 avg 4.9091\n",
+      "Game 420 Score 5.000 completed in   308 steps eps 0.100 last 10 avg 4.545\n",
+      "Game 430 Score 4.000 completed in   239 steps eps 0.100 last 10 avg 4.4552\n",
+      "Game 440 Score 3.000 completed in   327 steps eps 0.100 last 10 avg 5.000\n",
+      "Game 450 Score 8.000 completed in   308 steps eps 0.100 last 10 avg 5.818\n",
+      "Game 460 Score 8.000 completed in   332 steps eps 0.100 last 10 avg 5.727\n",
+      "Game 470 Score 6.000 completed in   368 steps eps 0.100 last 10 avg 5.364\n",
+      "Game 480 Score 5.000 completed in   325 steps eps 0.100 last 10 avg 4.455\n",
+      "Game 490 Score 5.000 completed in   294 steps eps 0.100 last 10 avg 4.545\n",
+      "Game 500 Score 3.000 completed in   218 steps eps 0.100 last 10 avg 4.455\n",
+      "Game 510 Score 4.000 completed in   268 steps eps 0.100 last 10 avg 3.455\n",
+      "Game 520 Score 1.000 completed in   168 steps eps 0.100 last 10 avg 5.000\n",
+      "Game 530 Score 8.000 completed in   322 steps eps 0.100 last 10 avg 5.0910\n",
+      "Game 540 Score 4.000 completed in   334 steps eps 0.100 last 10 avg 4.636\n",
+      "Game 550 Score 3.000 completed in   286 steps eps 0.100 last 10 avg 4.545\n",
+      "Game 560 Score 4.000 completed in   253 steps eps 0.100 last 10 avg 3.273\n",
+      "Game 570 Score 3.000 completed in   244 steps eps 0.100 last 10 avg 4.364\n",
+      "Game 580 Score 6.000 completed in   355 steps eps 0.100 last 10 avg 5.0918\n",
+      "Game 590 Score 8.000 completed in   326 steps eps 0.100 last 10 avg 5.0910\n",
+      "Game 600 Score 11.000 completed in   721 steps eps 0.100 last 10 avg 5.091\n",
+      "Game 610 Score 4.000 completed in   281 steps eps 0.100 last 10 avg 5.0918\n",
+      "Game 620 Score 8.000 completed in   293 steps eps 0.100 last 10 avg 5.182\n",
+      "Game 630 Score 8.000 completed in   472 steps eps 0.100 last 10 avg 5.7277\n",
+      "Game 640 Score 6.000 completed in   475 steps eps 0.100 last 10 avg 4.273\n",
+      "Game 650 Score 3.000 completed in   228 steps eps 0.100 last 10 avg 3.636\n",
+      "Game 660 Score 4.000 completed in   244 steps eps 0.100 last 10 avg 4.091\n",
+      "Game 670 Score 5.000 completed in   357 steps eps 0.100 last 10 avg 6.0919\n",
+      "Game 680 Score 4.000 completed in   287 steps eps 0.100 last 10 avg 4.455\n",
+      "Game 690 Score 3.000 completed in   235 steps eps 0.100 last 10 avg 3.364\n",
+      "Game 700 Score 4.000 completed in   243 steps eps 0.100 last 10 avg 4.000\n",
+      "Game 710 Score 6.000 completed in   404 steps eps 0.100 last 10 avg 4.818\n",
+      "Game 720 Score 5.000 completed in   311 steps eps 0.100 last 10 avg 4.455\n",
+      "Game 730 Score 5.000 completed in   333 steps eps 0.100 last 10 avg 4.273\n",
+      "Game 740 Score 5.000 completed in   325 steps eps 0.100 last 10 avg 4.455\n",
+      "Game 750 Score 2.000 completed in   181 steps eps 0.100 last 10 avg 4.273\n",
+      "Game 760 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 4.3649\n",
+      "Game 770 Score 11.000 completed in   445 steps eps 0.100 last 10 avg 5.636\n",
+      "Game 780 Score 4.000 completed in   243 steps eps 0.100 last 10 avg 5.545\n",
+      "Game 790 Score 3.000 completed in   246 steps eps 0.100 last 10 avg 4.6365\n",
+      "Game 800 Score 4.000 completed in   318 steps eps 0.100 last 10 avg 4.636\n",
+      "Game 810 Score 4.000 completed in   275 steps eps 0.100 last 10 avg 4.364\n",
+      "Game 820 Score 3.000 completed in   224 steps eps 0.100 last 10 avg 4.9090\n",
+      "Game 830 Score 3.000 completed in   209 steps eps 0.100 last 10 avg 4.364\n",
+      "Game 840 Score 3.000 completed in   221 steps eps 0.100 last 10 avg 5.636\n",
+      "Game 850 Score 5.000 completed in   288 steps eps 0.100 last 10 avg 4.909\n",
+      "Game 860 Score 7.000 completed in   250 steps eps 0.100 last 10 avg 4.182\n",
+      "Game 870 Score 6.000 completed in   340 steps eps 0.100 last 10 avg 4.818\n",
+      "Game 880 Score 6.000 completed in   339 steps eps 0.100 last 10 avg 4.455\n",
+      "Game 890 Score 5.000 completed in   315 steps eps 0.100 last 10 avg 4.636\n",
+      "Game 900 Score 10.000 completed in   434 steps eps 0.100 last 10 avg 4.818\n",
+      "Game 910 Score 5.000 completed in   289 steps eps 0.100 last 10 avg 4.636\n",
+      "Game 920 Score 7.000 completed in   270 steps eps 0.100 last 10 avg 4.818\n",
+      "Game 930 Score 5.000 completed in   344 steps eps 0.100 last 10 avg 4.727\n",
+      "Game 940 Score 8.000 completed in   481 steps eps 0.100 last 10 avg 4.455\n",
+      "Game 950 Score 4.000 completed in   258 steps eps 0.100 last 10 avg 4.909\n",
+      "Game 960 Score 3.000 completed in   246 steps eps 0.100 last 10 avg 5.3647\n",
+      "Game 970 Score 5.000 completed in   272 steps eps 0.100 last 10 avg 5.4558\n",
+      "Game 980 Score 6.000 completed in   300 steps eps 0.100 last 10 avg 4.182\n",
+      "Game 990 Score 5.000 completed in   320 steps eps 0.100 last 10 avg 4.273\n",
+      "Game 1000 Score 3.000 completed in   227 steps eps 0.100 last 10 avg 5.636\n",
+      "Game 1010 Score 9.000 completed in   334 steps eps 0.100 last 10 avg 5.6361\n",
+      "Game 1020 Score 5.000 completed in   289 steps eps 0.100 last 10 avg 7.0916\n",
+      "Game 1030 Score 7.000 completed in   351 steps eps 0.100 last 10 avg 5.455\n",
+      "Game 1040 Score 3.000 completed in   229 steps eps 0.100 last 10 avg 5.5454\n",
+      "Game 1050 Score 4.000 completed in   281 steps eps 0.100 last 10 avg 4.000\n",
+      "Game 1060 Score 4.000 completed in   434 steps eps 0.100 last 10 avg 4.727\n",
+      "Game 1070 Score 5.000 completed in   304 steps eps 0.100 last 10 avg 5.273\n",
+      "Game 1080 Score 10.000 completed in   496 steps eps 0.100 last 10 avg 5.000\n"
      ]
     },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Game 1090 Score 7.000 completed in   360 steps eps 0.100 last 10 avg 4.9092\n",
-      "Game 1100 Score 2.000 completed in   182 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 1110 Score 9.000 completed in   448 steps eps 0.100 last 10 avg 3.909\n",
-      "Game 1120 Score 5.000 completed in   365 steps eps 0.100 last 10 avg 3.455\n",
-      "Game 1130 Score 2.000 completed in   220 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 1140 Score 4.000 completed in   268 steps eps 0.100 last 10 avg 4.636\n",
-      "Game 1150 Score 3.000 completed in   250 steps eps 0.100 last 10 avg 4.545\n",
-      "Game 1160 Score 7.000 completed in   398 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 1170 Score 7.000 completed in   411 steps eps 0.100 last 10 avg 4.909\n",
-      "Game 1180 Score 9.000 completed in   425 steps eps 0.100 last 10 avg 5.818\n",
-      "Game 1190 Score 3.000 completed in   295 steps eps 0.100 last 10 avg 5.545\n",
-      "Game 1200 Score 4.000 completed in   292 steps eps 0.100 last 10 avg 3.909\n",
-      "Game 1210 Score 5.000 completed in   379 steps eps 0.100 last 10 avg 5.0916\n",
-      "Game 1220 Score 7.000 completed in   265 steps eps 0.100 last 10 avg 5.273\n",
-      "Game 1230 Score 2.000 completed in   183 steps eps 0.100 last 10 avg 4.091\n",
-      "Game 1240 Score 3.000 completed in   267 steps eps 0.100 last 10 avg 5.2734\n",
-      "Game 1250 Score 2.000 completed in   275 steps eps 0.100 last 10 avg 3.909\n",
-      "Game 1260 Score 8.000 completed in   442 steps eps 0.100 last 10 avg 6.5457\n",
-      "Game 1270 Score 7.000 completed in   388 steps eps 0.100 last 10 avg 5.273\n",
-      "Game 1280 Score 4.000 completed in   290 steps eps 0.100 last 10 avg 5.8183\n",
-      "Game 1290 Score 3.000 completed in   301 steps eps 0.100 last 10 avg 4.727\n",
-      "Game 1300 Score 3.000 completed in   277 steps eps 0.100 last 10 avg 4.4556\n",
-      "Game 1310 Score 7.000 completed in   281 steps eps 0.100 last 10 avg 4.6365\n",
-      "Game 1320 Score 2.000 completed in   372 steps eps 0.100 last 10 avg 5.727\n",
-      "Game 1330 Score 4.000 completed in   267 steps eps 0.100 last 10 avg 4.636\n",
-      "Game 1340 Score 4.000 completed in   257 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 1350 Score 4.000 completed in   238 steps eps 0.100 last 10 avg 3.727\n",
-      "Game 1360 Score 2.000 completed in   183 steps eps 0.100 last 10 avg 6.000\n",
-      "Game 1370 Score 2.000 completed in   200 steps eps 0.100 last 10 avg 3.818\n",
-      "Game 1380 Score 1.000 completed in   150 steps eps 0.100 last 10 avg 5.4554\n",
-      "Game 1390 Score 6.000 completed in   347 steps eps 0.100 last 10 avg 4.545\n",
-      "Game 1400 Score 8.000 completed in   377 steps eps 0.100 last 10 avg 6.7277\n",
-      "Game 1410 Score 4.000 completed in   289 steps eps 0.100 last 10 avg 5.091\n",
-      "Game 1420 Score 5.000 completed in   355 steps eps 0.100 last 10 avg 4.273\n",
-      "Game 1430 Score 5.000 completed in   393 steps eps 0.100 last 10 avg 5.364\n",
-      "Game 1440 Score 7.000 completed in   314 steps eps 0.100 last 10 avg 5.5454\n",
-      "Game 1450 Score 4.000 completed in   340 steps eps 0.100 last 10 avg 5.091\n",
-      "Game 1460 Score 7.000 completed in   486 steps eps 0.100 last 10 avg 5.7276\n",
-      "Game 1470 Score 3.000 completed in   228 steps eps 0.100 last 10 avg 4.909\n",
-      "Game 1480 Score 5.000 completed in   287 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 1490 Score 7.000 completed in   455 steps eps 0.100 last 10 avg 5.455\n",
-      "Game 1500 Score 3.000 completed in   266 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 1510 Score 6.000 completed in   376 steps eps 0.100 last 10 avg 4.909\n",
-      "Game 1520 Score 5.000 completed in   393 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 1530 Score 4.000 completed in   342 steps eps 0.100 last 10 avg 5.091\n",
-      "Game 1540 Score 9.000 completed in   362 steps eps 0.100 last 10 avg 4.727\n",
-      "Game 1550 Score 7.000 completed in   382 steps eps 0.100 last 10 avg 4.818\n",
-      "Game 1560 Score 4.000 completed in   250 steps eps 0.100 last 10 avg 4.818\n",
-      "Game 1570 Score 5.000 completed in   357 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 1580 Score 8.000 completed in   370 steps eps 0.100 last 10 avg 4.4551\n",
-      "Game 1590 Score 4.000 completed in   376 steps eps 0.100 last 10 avg 5.8185\n",
-      "Game 1600 Score 4.000 completed in   291 steps eps 0.100 last 10 avg 4.000\n",
-      "Game 1610 Score 7.000 completed in   263 steps eps 0.100 last 10 avg 5.7275\n",
-      "Game 1620 Score 8.000 completed in   402 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 1630 Score 3.000 completed in   227 steps eps 0.100 last 10 avg 5.9092\n",
-      "Game 1640 Score 4.000 completed in   262 steps eps 0.100 last 10 avg 4.182\n",
-      "Game 1650 Score 3.000 completed in   333 steps eps 0.100 last 10 avg 4.091\n",
-      "Game 1660 Score 3.000 completed in   228 steps eps 0.100 last 10 avg 4.182\n",
-      "Game 1670 Score 4.000 completed in   473 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 1680 Score 3.000 completed in   314 steps eps 0.100 last 10 avg 4.182\n",
-      "Game 1690 Score 7.000 completed in   285 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 1700 Score 5.000 completed in   336 steps eps 0.100 last 10 avg 5.091\n",
-      "Game 1710 Score 6.000 completed in   423 steps eps 0.100 last 10 avg 4.636\n",
-      "Game 1720 Score 6.000 completed in   350 steps eps 0.100 last 10 avg 6.4553\n",
-      "Game 1730 Score 7.000 completed in   257 steps eps 0.100 last 10 avg 5.818\n",
-      "Game 1740 Score 4.000 completed in   265 steps eps 0.100 last 10 avg 5.4550\n",
-      "Game 1750 Score 4.000 completed in   300 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 1760 Score 7.000 completed in   455 steps eps 0.100 last 10 avg 5.6368\n",
-      "Game 1770 Score 4.000 completed in   354 steps eps 0.100 last 10 avg 4.727\n",
-      "Game 1780 Score 5.000 completed in   295 steps eps 0.100 last 10 avg 5.273\n",
-      "Game 1790 Score 7.000 completed in   343 steps eps 0.100 last 10 avg 4.8185\n",
-      "Game 1800 Score 6.000 completed in   390 steps eps 0.100 last 10 avg 5.545\n",
-      "Game 1810 Score 4.000 completed in   287 steps eps 0.100 last 10 avg 4.273\n",
-      "Game 1820 Score 4.000 completed in   295 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 1830 Score 3.000 completed in   209 steps eps 0.100 last 10 avg 3.909\n",
-      "Game 1840 Score 4.000 completed in   309 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 1850 Score 7.000 completed in   422 steps eps 0.100 last 10 avg 5.7273\n",
-      "Game 1860 Score 8.000 completed in   388 steps eps 0.100 last 10 avg 6.0005\n",
-      "Game 1870 Score 3.000 completed in   310 steps eps 0.100 last 10 avg 4.364\n",
-      "Game 1880 Score 3.000 completed in   237 steps eps 0.100 last 10 avg 4.545\n",
-      "Game 1890 Score 4.000 completed in   255 steps eps 0.100 last 10 avg 5.3641\n",
-      "Game 1900 Score 4.000 completed in   266 steps eps 0.100 last 10 avg 5.4556\n",
-      "Game 1910 Score 3.000 completed in   311 steps eps 0.100 last 10 avg 3.455\n",
-      "Game 1920 Score 9.000 completed in   435 steps eps 0.100 last 10 avg 4.909\n",
-      "Game 1930 Score 4.000 completed in   254 steps eps 0.100 last 10 avg 5.182\n",
-      "Game 1940 Score 3.000 completed in   374 steps eps 0.100 last 10 avg 4.636\n",
-      "Game 1950 Score 7.000 completed in   253 steps eps 0.100 last 10 avg 5.6367\n",
-      "Game 1960 Score 3.000 completed in   226 steps eps 0.100 last 10 avg 4.727\n",
-      "Game 1970 Score 6.000 completed in   344 steps eps 0.100 last 10 avg 5.7275\n",
-      "Game 1980 Score 10.000 completed in   609 steps eps 0.100 last 10 avg 6.545\n",
-      "Game 1990 Score 6.000 completed in   351 steps eps 0.100 last 10 avg 4.455\n",
-      "Game 2000 Score 1.000 completed in   168 steps eps 0.100 last 10 avg 4.4558\n",
-      "Game 2010 Score 3.000 completed in   291 steps eps 0.100 last 10 avg 3.091\n",
-      "Game 2020 Score 5.000 completed in   358 steps eps 0.100 last 10 avg 4.727\n",
-      "Game 2030 Score 4.000 completed in   298 steps eps 0.100 last 10 avg 4.091\n",
-      "Game 2040 Score 5.000 completed in   403 steps eps 0.100 last 10 avg 5.3648\n",
-      "Game 2050 Score 6.000 completed in   330 steps eps 0.100 last 10 avg 5.6362\n",
-      "Game 2060 Score 6.000 completed in   321 steps eps 0.100 last 10 avg 5.091\n",
-      "Game 2070 Score 11.000 completed in   402 steps eps 0.100 last 10 avg 6.000\n",
-      "Game 2080 Score 4.000 completed in   279 steps eps 0.100 last 10 avg 5.5452\n",
-      "Game 2090 Score 3.000 completed in   225 steps eps 0.100 last 10 avg 3.818\n",
-      "Game 2100 Score 3.000 completed in   242 steps eps 0.100 last 10 avg 4.6361\n",
-      "Game 2110 Score 5.000 completed in   341 steps eps 0.100 last 10 avg 4.182\n",
-      "Game 2120 Score 7.000 completed in   328 steps eps 0.100 last 10 avg 4.545\n",
-      "Game 2130 Score 5.000 completed in   285 steps eps 0.100 last 10 avg 6.0002\n",
-      "Game 2140 Score 7.000 completed in   443 steps eps 0.100 last 10 avg 4.727\n",
-      "Game 2150 Score 4.000 completed in   351 steps eps 0.100 last 10 avg 5.364\n",
-      "Game 2160 Score 5.000 completed in   417 steps eps 0.100 last 10 avg 6.0007\n"
+      "Game 1090 Score 3.000 completed in   228 steps eps 0.100 last 10 avg 4.636\n",
+      "Game 1100 Score 4.000 completed in   242 steps eps 0.100 last 10 avg 5.182\n",
+      "Game 1110 Score 8.000 completed in   448 steps eps 0.100 last 10 avg 6.0008\n",
+      "Game 1120 Score 8.000 completed in   470 steps eps 0.100 last 10 avg 6.1821\n",
+      "Game 1130 Score 5.000 completed in   288 steps eps 0.100 last 10 avg 6.3646\n",
+      "Game 1140 Score 6.000 completed in   373 steps eps 0.100 last 10 avg 4.182\n",
+      "Game 1150 Score 8.000 completed in   291 steps eps 0.100 last 10 avg 4.364\n",
+      "Game 1160 Score 2.000 completed in   179 steps eps 0.100 last 10 avg 4.364\n",
+      "Game 1170 Score 5.000 completed in   312 steps eps 0.100 last 10 avg 5.0001\n",
+      "Game 1180 Score 4.000 completed in   256 steps eps 0.100 last 10 avg 4.9095\n",
+      "Game 1190 Score 7.000 completed in   474 steps eps 0.100 last 10 avg 5.0910\n",
+      "Game 1200 Score 3.000 completed in   209 steps eps 0.100 last 10 avg 5.3644\n",
+      "Game 1210 Score 3.000 completed in   213 steps eps 0.100 last 10 avg 6.3648\n",
+      "Game 1220 Score 1.000 completed in   151 steps eps 0.100 last 10 avg 5.0918\n",
+      "Game 1230 Score 6.000 completed in   435 steps eps 0.100 last 10 avg 5.636\n",
+      "Game 1240 Score 3.000 completed in   228 steps eps 0.100 last 10 avg 5.7278\n",
+      "Game 1250 Score 9.000 completed in   457 steps eps 0.100 last 10 avg 4.182\n",
+      "Game 1260 Score 7.000 completed in   250 steps eps 0.100 last 10 avg 5.8184\n",
+      "Game 1270 Score 2.000 completed in   199 steps eps 0.100 last 10 avg 4.364\n",
+      "Game 1280 Score 6.000 completed in   336 steps eps 0.100 last 10 avg 3.545\n",
+      "Game 1290 Score 5.000 completed in   307 steps eps 0.100 last 10 avg 5.455\n",
+      "Game 1300 Score 4.000 completed in   272 steps eps 0.100 last 10 avg 4.727\n",
+      "Game 1310 Score 5.000 completed in   287 steps eps 0.100 last 10 avg 4.364\n",
+      "Game 1320 Score 5.000 completed in   287 steps eps 0.100 last 10 avg 6.3645\n",
+      "Game 1330 Score 3.000 completed in   246 steps eps 0.100 last 10 avg 5.727\n",
+      "Game 1340 Score 3.000 completed in   227 steps eps 0.100 last 10 avg 5.364\n",
+      "Game 1350 Score 3.000 completed in   225 steps eps 0.100 last 10 avg 4.727\n",
+      "Game 1360 Score 4.000 completed in   236 steps eps 0.100 last 10 avg 5.000\n",
+      "Game 1370 Score 4.000 completed in   261 steps eps 0.100 last 10 avg 4.636\n",
+      "Game 1380 Score 7.000 completed in   369 steps eps 0.100 last 10 avg 5.455\n",
+      "Game 1390 Score 13.000 completed in   418 steps eps 0.100 last 10 avg 6.182\n",
+      "Game 1400 Score 8.000 completed in   458 steps eps 0.100 last 10 avg 7.1828\n",
+      "Game 1410 Score 7.000 completed in   409 steps eps 0.100 last 10 avg 6.000\n",
+      "Game 1420 Score 5.000 completed in   325 steps eps 0.100 last 10 avg 5.273\n",
+      "Game 1430 Score 11.000 completed in   442 steps eps 0.100 last 10 avg 6.909\n",
+      "Game 1440 Score 5.000 completed in   272 steps eps 0.100 last 10 avg 7.0918\n",
+      "Game 1450 Score 4.000 completed in   263 steps eps 0.100 last 10 avg 5.7279\n",
+      "Game 1460 Score 8.000 completed in   414 steps eps 0.100 last 10 avg 4.909\n",
+      "Game 1470 Score 5.000 completed in   275 steps eps 0.100 last 10 avg 5.9096\n",
+      "Game 1480 Score 4.000 completed in   257 steps eps 0.100 last 10 avg 5.0914\n",
+      "Game 1490 Score 5.000 completed in   377 steps eps 0.100 last 10 avg 6.5456\n",
+      "Game 1500 Score 8.000 completed in   424 steps eps 0.100 last 10 avg 5.8188\n",
+      "Game 1510 Score 5.000 completed in   296 steps eps 0.100 last 10 avg 5.8183\n",
+      "Game 1520 Score 6.000 completed in   329 steps eps 0.100 last 10 avg 4.9095\n",
+      "Game 1530 Score 5.000 completed in   289 steps eps 0.100 last 10 avg 6.0914\n",
+      "Game 1540 Score 6.000 completed in   358 steps eps 0.100 last 10 avg 5.545\n",
+      "Game 1550 Score 2.000 completed in   183 steps eps 0.100 last 10 avg 4.909\n",
+      "Game 1560 Score 4.000 completed in   241 steps eps 0.100 last 10 avg 5.455\n",
+      "Game 1570 Score 6.000 completed in   377 steps eps 0.100 last 10 avg 4.455\n",
+      "Game 1580 Score 6.000 completed in   339 steps eps 0.100 last 10 avg 5.1820\n",
+      "Game 1590 Score 7.000 completed in   372 steps eps 0.100 last 10 avg 5.091\n",
+      "Game 1600 Score 5.000 completed in   287 steps eps 0.100 last 10 avg 6.0915\n",
+      "Game 1610 Score 7.000 completed in   358 steps eps 0.100 last 10 avg 5.273\n",
+      "Game 1620 Score 5.000 completed in   323 steps eps 0.100 last 10 avg 5.000\n",
+      "Game 1630 Score 5.000 completed in   310 steps eps 0.100 last 10 avg 6.091\n",
+      "Game 1640 Score 7.000 completed in   429 steps eps 0.100 last 10 avg 6.8189\n",
+      "Game 1650 Score 8.000 completed in   286 steps eps 0.100 last 10 avg 4.455\n",
+      "Game 1660 Score 4.000 completed in   268 steps eps 0.100 last 10 avg 6.3648\n",
+      "Game 1670 Score 4.000 completed in   295 steps eps 0.100 last 10 avg 5.7277\n",
+      "Game 1680 Score 5.000 completed in   307 steps eps 0.100 last 10 avg 4.4553\n",
+      "Game 1690 Score 4.000 completed in   261 steps eps 0.100 last 10 avg 5.9095\n",
+      "Game 1700 Score 6.000 completed in   340 steps eps 0.100 last 10 avg 6.1827\n",
+      "Game 1710 Score 9.000 completed in   326 steps eps 0.100 last 10 avg 7.2737\n",
+      "Game 1720 Score 4.000 completed in   242 steps eps 0.100 last 10 avg 5.455\n",
+      "Game 1730 Score 6.000 completed in   337 steps eps 0.100 last 10 avg 5.0001\n",
+      "Game 1740 Score 4.000 completed in   300 steps eps 0.100 last 10 avg 5.727\n",
+      "Game 1750 Score 9.000 completed in   411 steps eps 0.100 last 10 avg 6.3642\n",
+      "Game 1760 Score 8.000 completed in   418 steps eps 0.100 last 10 avg 6.091\n",
+      "Game 1770 Score 9.000 completed in   436 steps eps 0.100 last 10 avg 6.455\n",
+      "Game 1780 Score 6.000 completed in   381 steps eps 0.100 last 10 avg 5.818\n",
+      "Game 1790 Score 7.000 completed in   367 steps eps 0.100 last 10 avg 5.091\n",
+      "Game 1800 Score 10.000 completed in   485 steps eps 0.100 last 10 avg 6.727\n",
+      "Game 1810 Score 6.000 completed in   397 steps eps 0.100 last 10 avg 5.545\n",
+      "Game 1820 Score 6.000 completed in   377 steps eps 0.100 last 10 avg 6.2739\n",
+      "Game 1830 Score 8.000 completed in   427 steps eps 0.100 last 10 avg 5.727\n",
+      "Game 1840 Score 5.000 completed in   285 steps eps 0.100 last 10 avg 7.1826\n",
+      "Game 1850 Score 3.000 completed in   225 steps eps 0.100 last 10 avg 5.4551\n",
+      "Game 1860 Score 5.000 completed in   285 steps eps 0.100 last 10 avg 5.091\n",
+      "Game 1870 Score 6.000 completed in   379 steps eps 0.100 last 10 avg 5.364\n",
+      "Game 1880 Score 5.000 completed in   290 steps eps 0.100 last 10 avg 6.0003\n",
+      "Game 1890 Score 6.000 completed in   306 steps eps 0.100 last 10 avg 4.545\n",
+      "Game 1900 Score 5.000 completed in   291 steps eps 0.100 last 10 avg 6.000\n",
+      "Game 1910 Score 5.000 completed in   288 steps eps 0.100 last 10 avg 5.182\n",
+      "Game 1920 Score 3.000 completed in   229 steps eps 0.100 last 10 avg 4.818\n",
+      "Game 1930 Score 5.000 completed in   290 steps eps 0.100 last 10 avg 5.000\n",
+      "Game 1940 Score 8.000 completed in   437 steps eps 0.100 last 10 avg 6.5457\n",
+      "Game 1950 Score 4.000 completed in   244 steps eps 0.100 last 10 avg 6.545\n",
+      "Game 1960 Score 4.000 completed in   303 steps eps 0.100 last 10 avg 6.0916\n",
+      "Game 1970 Score 5.000 completed in   308 steps eps 0.100 last 10 avg 4.818\n",
+      "Game 1980 Score 3.000 completed in   210 steps eps 0.100 last 10 avg 4.273\n",
+      "Game 1990 Score 4.000 completed in   263 steps eps 0.100 last 10 avg 4.8185\n",
+      "Game 2000 Score 6.000 completed in   374 steps eps 0.100 last 10 avg 5.636\n",
+      "Game 2010 Score 7.000 completed in   247 steps eps 0.100 last 10 avg 4.8188\n",
+      "Game 2020 Score 7.000 completed in   268 steps eps 0.100 last 10 avg 6.545\n",
+      "Game 2030 Score 6.000 completed in   317 steps eps 0.100 last 10 avg 4.909\n",
+      "Game 2040 Score 10.000 completed in   448 steps eps 0.100 last 10 avg 5.727\n",
+      "Game 2050 Score 8.000 completed in   439 steps eps 0.100 last 10 avg 5.909\n",
+      "Game 2060 Score 5.000 completed in   290 steps eps 0.100 last 10 avg 6.0007\n",
+      "Game 2070 Score 3.000 completed in   230 steps eps 0.100 last 10 avg 4.545\n",
+      "Game 2080 Score 4.000 completed in   241 steps eps 0.100 last 10 avg 4.000\n",
+      "Game 2090 Score 9.000 completed in   326 steps eps 0.100 last 10 avg 6.0007\n",
+      "Game 2100 Score 2.000 completed in   263 steps eps 0.100 last 10 avg 5.273\n",
+      "Game 2110 Score 5.000 completed in   346 steps eps 0.100 last 10 avg 4.545\n",
+      "Game 2120 Score 6.000 completed in   321 steps eps 0.100 last 10 avg 5.000\n",
+      "Game 2130 Score 6.000 completed in   328 steps eps 0.100 last 10 avg 6.8183\n",
+      "Game 2140 Score 4.000 completed in   258 steps eps 0.100 last 10 avg 4.364\n",
+      "Game 2150 Score 5.000 completed in   327 steps eps 0.100 last 10 avg 4.636\n",
+      "Game 2160 Score 3.000 completed in   211 steps eps 0.100 last 10 avg 5.9091\n"
      ]
     },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Game 2170 Score 4.000 completed in   333 steps eps 0.100 last 10 avg 5.182\n",
-      "Game 2180 Score 7.000 completed in   418 steps eps 0.100 last 10 avg 4.909\n",
-      "Game 2190 Score 8.000 completed in   295 steps eps 0.100 last 10 avg 5.9092\n",
-      "Game 2200 Score 3.000 completed in   214 steps eps 0.100 last 10 avg 4.818\n",
-      "Game 2210 Score 4.000 completed in   238 steps eps 0.100 last 10 avg 3.909\n",
-      "Game 2220 Score 8.000 completed in   412 steps eps 0.100 last 10 avg 6.8185\n",
-      "Game 2230 Score 4.000 completed in   366 steps eps 0.100 last 10 avg 4.727\n",
-      "Game 2240 Score 5.000 completed in   307 steps eps 0.100 last 10 avg 4.273\n",
-      "Game 2250 Score 2.000 completed in   251 steps eps 0.100 last 10 avg 2.818\n",
-      "Game 2260 Score 4.000 completed in   330 steps eps 0.100 last 10 avg 3.545\n",
-      "Game 2270 Score 3.000 completed in   395 steps eps 0.100 last 10 avg 6.1821\n",
-      "Game 2280 Score 4.000 completed in   260 steps eps 0.100 last 10 avg 5.091\n",
-      "Game 2290 Score 4.000 completed in   319 steps eps 0.100 last 10 avg 3.727\n",
-      "Game 2300 Score 5.000 completed in   320 steps eps 0.100 last 10 avg 4.0000\n",
-      "Game 2310 Score 3.000 completed in   240 steps eps 0.100 last 10 avg 6.3649\n",
-      "Game 2320 Score 12.000 completed in   351 steps eps 0.100 last 10 avg 6.182\n",
-      "Game 2330 Score 4.000 completed in   369 steps eps 0.100 last 10 avg 6.1829\n",
-      "Game 2340 Score 3.000 completed in   253 steps eps 0.100 last 10 avg 4.000\n",
-      "Game 2350 Score 3.000 completed in   448 steps eps 0.100 last 10 avg 3.182\n",
-      "Game 2360 Score 9.000 completed in   438 steps eps 0.100 last 10 avg 5.4555\n",
-      "Game 2370 Score 5.000 completed in   299 steps eps 0.100 last 10 avg 3.909\n",
-      "Game 2380 Score 2.000 completed in   384 steps eps 0.100 last 10 avg 4.545\n",
-      "Game 2390 Score 7.000 completed in   258 steps eps 0.100 last 10 avg 5.545\n",
-      "Game 2400 Score 2.000 completed in   253 steps eps 0.100 last 10 avg 4.000\n",
-      "Game 2410 Score 3.000 completed in   266 steps eps 0.100 last 10 avg 4.636\n",
-      "Game 2420 Score 13.000 completed in   336 steps eps 0.100 last 10 avg 5.545\n",
-      "Game 2430 Score 4.000 completed in   297 steps eps 0.100 last 10 avg 5.727\n",
-      "Game 2440 Score 9.000 completed in   313 steps eps 0.100 last 10 avg 5.000\n",
-      "Game 2450 Score 3.000 completed in   293 steps eps 0.100 last 10 avg 6.0007\n",
-      "Game 2460 Score 3.000 completed in   331 steps eps 0.100 last 10 avg 4.3645\n",
-      "Game 2470 Score 12.000 completed in   322 steps eps 0.100 last 10 avg 5.364\n",
-      "Game 2480 Score 4.000 completed in   286 steps eps 0.100 last 10 avg 5.091\n",
-      "Game 2490 Score 7.000 completed in   279 steps eps 0.100 last 10 avg 4.636\n",
-      "Game 2499 Score 11.000 completed in   492 steps eps 0.100 last 10 avg 5.273"
+      "Game 2170 Score 5.000 completed in   305 steps eps 0.100 last 10 avg 4.727\n",
+      "Game 2180 Score 10.000 completed in   445 steps eps 0.100 last 10 avg 5.000\n",
+      "Game 2190 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 5.636\n",
+      "Game 2200 Score 10.000 completed in   402 steps eps 0.100 last 10 avg 5.545\n",
+      "Game 2210 Score 5.000 completed in   320 steps eps 0.100 last 10 avg 5.273\n",
+      "Game 2220 Score 3.000 completed in   209 steps eps 0.100 last 10 avg 4.000\n",
+      "Game 2230 Score 7.000 completed in   492 steps eps 0.100 last 10 avg 4.818\n",
+      "Game 2240 Score 4.000 completed in   241 steps eps 0.100 last 10 avg 5.364\n",
+      "Game 2250 Score 5.000 completed in   307 steps eps 0.100 last 10 avg 5.1823\n",
+      "Game 2260 Score 4.000 completed in   241 steps eps 0.100 last 10 avg 6.0914\n",
+      "Game 2270 Score 8.000 completed in   321 steps eps 0.100 last 10 avg 5.8185\n",
+      "Game 2280 Score 8.000 completed in   396 steps eps 0.100 last 10 avg 6.091\n",
+      "Game 2290 Score 5.000 completed in   350 steps eps 0.100 last 10 avg 6.3644\n",
+      "Game 2300 Score 4.000 completed in   254 steps eps 0.100 last 10 avg 4.545\n",
+      "Game 2310 Score 4.000 completed in   243 steps eps 0.100 last 10 avg 5.4556\n",
+      "Game 2320 Score 10.000 completed in   463 steps eps 0.100 last 10 avg 5.182\n",
+      "Game 2330 Score 6.000 completed in   321 steps eps 0.100 last 10 avg 6.273\n",
+      "Game 2340 Score 3.000 completed in   208 steps eps 0.100 last 10 avg 5.818\n",
+      "Game 2350 Score 11.000 completed in   431 steps eps 0.100 last 10 avg 5.545\n",
+      "Game 2360 Score 1.000 completed in   150 steps eps 0.100 last 10 avg 5.818\n",
+      "Game 2370 Score 7.000 completed in   344 steps eps 0.100 last 10 avg 6.3643\n",
+      "Game 2380 Score 8.000 completed in   405 steps eps 0.100 last 10 avg 6.273\n",
+      "Game 2390 Score 5.000 completed in   370 steps eps 0.100 last 10 avg 6.273\n",
+      "Game 2398 Score 8.000 completed in   421 steps eps 0.100 last 10 avg 7.1829"
      ]
     }
    ],
@@ -1390,7 +1382,7 @@
     "action_size = env.action_space.n\n",
     "state = env.reset()\n",
     "height, width = preprocess(state).shape\n",
-    "EPS_DECAY  = 0.99995\n",
+    "EPS_DECAY  = 0.9999\n",
     "EPS_MIN = 0.1\n",
     "\n",
     "# if gpu is to be used\n",
@@ -1438,17 +1430,9 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": null,
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "  1\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
     "import torch\n",
     "v=torch.LongTensor([1])\n",
diff --git a/README.md b/README.md
index 29a6b9c..e92c8bd 100644
--- a/README.md
+++ b/README.md
@@ -48,7 +48,7 @@ Let's take a step back to see where Deep Reinforcement Learning fits in the arti
 
 ### Supervised Learning
 Supervised learning is the most commonly used approach. In supervised learning, a set of labeled training examples is provided as input. The goal of the system is to generate a function that best captures the relationship between the input features and the known **truth** values. An example might be to estimate whether or not your car needs maintenance based on all of the input sensors. In this case, expert mechanics might be asked to look at the sensor inputs and offer their judgements as to whether or not maintenance is required. At its most basic, an example of supervised learning would be a simple linear regression.
-<p><img src="svgs/b60e8a217b0c7ceff66266cf1724b261.svg" align=middle width=75.79696575pt height=14.611878599999999pt/></p>
+<p align="center"><img src="svgs/b60e8a217b0c7ceff66266cf1724b261.svg?invert_in_darkmode" align=middle width=75.79696575pt height=14.611878599999999pt/></p>
 
 ### Unsupervised Learning
 Unsupervised learning is, in contrast, where the system learns to represent the data in the absence of any truth values. What is this representation? Going back to the car analogy, the task would no longer be to identify something actionable like whether the car needs maintenance. Instead, the unsupervised learning system might form similar models of "cars" based on these sensor inputs. In the ideal case, maybe the system even learns some model of a car that needs maintenance (though it would not be able to tell you that's what it learned). 
@@ -71,33 +71,33 @@ Deep learning has had phenomenal success in recent years. There are a few reason
 - Growth in computing power and the [application of GPUs to neural networks](http://www.machinelearning.org/archive/icml2009/papers/218.pdf).
 
 The math of the network is relatively straightforward. At a given layer, each node has connections to some subset of the nodes in the next layer. Each of these connections is given some weight. The input value to a given node at a given layer is the summation of the value of the source node for each connection multiplied by the weight assigned to that connection. We can consider the weights for a given layer as a vector and then perform a matrix multiplication of that weight vector by the input feature vector and add in a bias.
-<p><img src="/svgs/a51d6ed3c27c591046d842cb38a5ce3d.svg" align=middle width=256.47608415pt height=18.7598829pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/a51d6ed3c27c591046d842cb38a5ce3d.svg?invert_in_darkmode" align=middle width=256.47608415pt height=18.7598829pt/></p>
 where H is the number of hidden layers, W is the weight vector for each layer, and the zeroth layer is the input layer. 
 
 We need one additional feature in order for this generalized network to approximate the complex functions hidden within our data. The missing feature is an **activation function**. An activation function is a nonlinear function applied to the output of a given layer in the network. Without a nonlinear function, it is not possible for the network to represent anything other than linear functions and that would be quite disappointing for this post. 
 
 For the hidden layers, the most common activation function is the **ReLU** or **Re**ctified **L**inear **U**nit. This function is a simple piece-wise linear function and behaves as an on-off switch for a given unit. The function is absurdly simple and produces quite the obvious output.
-<p><img src="/svgs/c8c67fe5a5342cb4550611d170ad7b8b.svg" align=middle width=158.78659829999998pt height=16.438356pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/c8c67fe5a5342cb4550611d170ad7b8b.svg?invert_in_darkmode" align=middle width=158.78659829999998pt height=16.438356pt/></p>
 which looks like the following for arbitrary input x. 
 ![ReLU figure](relu.png  "ReLU Example")
 
 For the output layer, the activation function needs to map to your problem domain. For example, if the task is to produce a binomial classifier (think "dogs vs cats") then the sigmoid activation function might be useful.
-<p><img src="/svgs/76a3d962e4802415439e67ff2200775f.svg" align=middle width=110.56831829999999pt height=34.3600389pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/76a3d962e4802415439e67ff2200775f.svg?invert_in_darkmode" align=middle width=110.56831829999999pt height=34.3600389pt/></p>
 What this function does is it drives higher valued inputs to the value *1* and lower valued inputs to the value *0*. Maybe *1* maps to dogs and *0* maps to cats in your system. Then you can think of this function as producing a value that is the probability that the input is a dog.
 ![sigmoid example](sigmoid.png  "Sigmoid Example")
 
 Finally, after we have the structure of our network, we need the ability to train the network using backpropagation. The idea is that first we calculate the error in the network result versus a truth result. Then we walk backwards through the network adjusting weights where the delta in the weights is driven by the contribution of that node's weight to the total error. That is, we adjust the weight of a node by the gradient of the error versus the node weight multiplied by some constant learning rate.
 
-<p><img src="/svgs/d5489d010002ec96c8a0ee24edd75454.svg" align=middle width=115.45823519999999pt height=38.5152603pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/d5489d010002ec96c8a0ee24edd75454.svg?invert_in_darkmode" align=middle width=115.45823519999999pt height=38.5152603pt/></p>
 
 That's a little abstract. Let's work our way up to that by looking at a network with a single layer. The output of our single layer of weights multiplied by the input vector yields our predicted output. That prediction versus our truth value forms our error. We can find the contribution of each node in the output layer to our error by multiplying the **input** to that node versus that output error. 
-<p><img src="/svgs/345627b78a823d45c56557b6bd186eb1.svg" align=middle width=122.34118214999998pt height=13.881256950000001pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/345627b78a823d45c56557b6bd186eb1.svg?invert_in_darkmode" align=middle width=122.34118214999998pt height=13.881256950000001pt/></p>
 Why does this make sense? Consider the trivial example where the input to a node is 0. Then that node necessarily has no impact on the output. So, it would not make sense to adjust the weight of that node in response.
 
 That logic works well enough for a single layer but what about when we have multiple hidden layers? Consider the case where we set the learning rate to 1.0 in order to force the network to immediately match the output. In that case, if we simply used the logic outlined above at each hidden layer, we would overcompensate for the error and we still wouldn't match. So, instead of using the network output error at each layer multiplied by the input to that layer, we'll use the error from the next layer multiplied by the weights entering that layer. This is what is meant by backpropagation. The computation is run in reverse.
-<p><img src="/svgs/01d9d3e2c182d5c5332cdf74c9e03fc4.svg" align=middle width=93.95260545pt height=15.068469899999998pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/01d9d3e2c182d5c5332cdf74c9e03fc4.svg?invert_in_darkmode" align=middle width=93.95260545pt height=15.068469899999998pt/></p>
 an
-<p><img src="/svgs/1aadf9a3dcbdf10ef1d5aa28ff1b8ec2.svg" align=middle width=119.8944648pt height=15.068469899999998pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/1aadf9a3dcbdf10ef1d5aa28ff1b8ec2.svg?invert_in_darkmode" align=middle width=119.8944648pt height=15.068469899999998pt/></p>
 for some learning rate alpha.
  
 Finally, I'll note a major caveat with this description of backprop: I included only linear units for simplicity. Nonlinear units have their own gradients that must be properly accounted for. You would be justified to have concern about the programming effort required to implement all of this. Fortunately, [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) is there to save the day. I won't go into details on autograd here but I'll note that the pytorch framework tracks all computations leading to the output value without requiring any data management on your part. Included in that tracking is the gradient of each computation. Backprop is then safe and easy. 
@@ -119,18 +119,18 @@ We are going to "convolve" this image with a 2x2 *kernel*.
 	       [0, 1]]) 
 
 What we are going to do is take the dot product of each 2x2 section of the source image with the 2x2 kernel. For example, the pixel at the 0,0 position in the output image will have value of
-<p><img src="/svgs/33be1e37e2dbffbaa9bcd32fc06bd672.svg" align=middle width=665.0113024499999pt height=14.611878599999999pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/33be1e37e2dbffbaa9bcd32fc06bd672.svg?invert_in_darkmode" align=middle width=665.0113024499999pt height=14.611878599999999pt/></p>
 More generally
-<p><img src="/svgs/b64157238705afe319f2440d348cbfb4.svg" align=middle width=195.43486875pt height=38.89287435pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/b64157238705afe319f2440d348cbfb4.svg?invert_in_darkmode" align=middle width=195.43486875pt height=38.89287435pt/></p>
 where O is the output image and m and n are the respective offsets in the image. You can imagine this as sliding a 2x2 filter across the input matrix row by row until you have covered the entire image. Note that this operation can only be performed twice for each row in the matrix. More generally, as defined above we can establish the size of the output image in a given direction from the input image size in that direction and the kernel size in that direction:
-<p><img src="/svgs/3f0ca35850584b89085c967da1b8cbac.svg" align=middle width=188.77287165pt height=16.438356pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/3f0ca35850584b89085c967da1b8cbac.svg?invert_in_darkmode" align=middle width=188.77287165pt height=16.438356pt/></p>
 where the *K-1* comes from the fact that the overlap in the image will be the length of the kernel minus 1.
 
 This is referred to as "valid" padding. That is, every pixel in the output image is computed only using input image pixels and the kernel. Meanwhile, "same" padding is the alternative where enough zeroes are appended into the image in order that the output image has the same shape as the input image.
 
 What if, on the other hand, you decided that there was no need to perform this operation on every possible sub-matrix in the image? Instead, maybe you want to start with your m,n moving by two pixels at a time? That is called the *stride* of the convolution. As you might guess, that also goes into determining the shape of the output image leading to the final equation:
 
-<p><img src="/svgs/fc14b802a291084dac59e54594bf8617.svg" align=middle width=176.8249065pt height=16.438356pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/fc14b802a291084dac59e54594bf8617.svg?invert_in_darkmode" align=middle width=176.8249065pt height=16.438356pt/></p>
 
 where s is the stride and P has a value of 0 for valid padding and a value of 1 for same padding. 
 
@@ -172,7 +172,7 @@ And the resulting output image is
 Now that we've seen the convolution operation, how do we bring this back up to a convolutional neural network? The key is how do you decide what filters will actually be critical for the problem at hand. Rather than hand-coding a filter as I did above, we can have the network learn what filters to use. When we define a CNN, we specify to the network the shape of the kernel, the stride, the padding, and how many of these kernels to construct. Going back to the edge detection filter it is quite unlikely that the output of that convolution would be enough to solve our problem. But, the combined output of a bunch of different kernels with different values may actually do the trick. 
 
 Referring back to the input image we are using. The size of that image is 512x512. If we constructed 64 filters, then our output image will have 64 layers or channels. If we assume for the moment that we are using "same" padding and a stride of 1, then we'd produce 64 output images each of shape 512x512. More generally, we describe the shape as
-<p><img src="/svgs/66ddbc7ce01c21a2abb97b86256e98f5.svg" align=middle width=202.04611844999997pt height=14.611878599999999pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/66ddbc7ce01c21a2abb97b86256e98f5.svg?invert_in_darkmode" align=middle width=202.04611844999997pt height=14.611878599999999pt/></p>
 
 This also illustrates how you might handle conventional RGB image input. Each color is a channel. Tying this back to the earlier discussion of deep learning you can now see that the input to a given node in the network is influenced by some subset of the pixels from each of the input channels. Critically, that subset is driven by the kernel size meaning that the input into a node is driven only by those pixels near the current node in the 2d image. Finally, a CNN is just a particular network architecture. The basic rules about matrix multiplication on non-linearity still apply so we'll apply a ReLU activation function after the convolution.
 
@@ -183,22 +183,22 @@ With that, we are ready to move on to reinforcement learning and see how deep le
 Reinforcement learning predates deep learning and provides the framework into which we will insert deep learning. 
 
 First, let's define a policy as a function that maps some state into an action. 
-<p><img src="/svgs/2ea390c3a0b434f195d1024747119d30.svg" align=middle width=65.62374224999999pt height=7.0776222pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/2ea390c3a0b434f195d1024747119d30.svg?invert_in_darkmode" align=middle width=65.62374224999999pt height=7.0776222pt/></p>
 It is important to note that this relationship need not be deterministic. Now, let's define the *value* of some state with respect a given policy.
-<p><img src="/svgs/2f627abb3dfe30de7534c4cc9f439b07.svg" align=middle width=321.97857945pt height=45.2741091pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/2f627abb3dfe30de7534c4cc9f439b07.svg?invert_in_darkmode" align=middle width=321.97857945pt height=45.2741091pt/></p>
 What this is saying is that the value of a state can be expressed as the sum total of all expected future rewards while following a given policy.
 
 Let's make one change to the above equations where we introduce an exponential discount on future rewards. You can consider this the "a bird in the hand is worth two in the bush" extension.
-<p><img src="/svgs/6e59eda441709b61792cae2224f97177.svg" align=middle width=237.59733359999996pt height=45.2741091pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/6e59eda441709b61792cae2224f97177.svg?invert_in_darkmode" align=middle width=237.59733359999996pt height=45.2741091pt/></p>
 
 We can take this a bit farther and define the state-value function for a given policy.
-<p><img src="/svgs/ecf8acfcb7aa66d8e5e92a089e633e25.svg" align=middle width=307.3707846pt height=45.2741091pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/ecf8acfcb7aa66d8e5e92a089e633e25.svg?invert_in_darkmode" align=middle width=307.3707846pt height=45.2741091pt/></p>
 This is essentially a restatement of the earlier value function where we now include the expected return from a specific action while in state s. 
 
 Bringing this altogether, what we would like to find is the optimal policy.
-<p><img src="/svgs/82f5e02240e61c20fbb1cc44aefa6704.svg" align=middle width=72.98626335pt height=9.54335085pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/82f5e02240e61c20fbb1cc44aefa6704.svg?invert_in_darkmode" align=middle width=72.98626335pt height=9.54335085pt/></p>
 That optimal policy would be the one that returns the action that maximizes the state value function.
-<p><img src="/svgs/f4093225512fe51d141558cbeb9c9765.svg" align=middle width=175.72122645pt height=16.438356pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/f4093225512fe51d141558cbeb9c9765.svg?invert_in_darkmode" align=middle width=175.72122645pt height=16.438356pt/></p>
 How might we find such an optimal policy? The obvious answer would be to play all possible games while tracking the ultimate game score received for taking actions at a given state. Then, the optimal action is merely the action that maximizes the mean game score from taking the action. That is clearly not a viable option for most problems that we would be interested in. We can approximate that value function by executing a monte carlo simulation but that would still fall apart under virtually all games of interest. See the breakout game above. The input is 210x164x3 meaning that the state space is a 110800 dimensional vector where each unit has a range of 256 possible values yielding over 25 million unique input images. That is the input state size for a given timestep and, as noted earlier, there may be thousands or tens of thousands of timesteps for a single game. It is simply infeasible to approximate the function in this manner.
 
 This is where deep learning comes to the rescue. We will use deep learning to learn a function that approximates that mapping from the input state to value.
@@ -209,9 +209,9 @@ The paper introducing Deep Q Networks or DQN uses a convolutional neural network
 ### DQN Algorithm
 ![DQN Algorithm](dqn_algorithm.png  "DQN Algorithm")
 There are a few techniques embedded in this algorithm that allow it to work. First, note that the algorithm uses a "replay buffer" where previous records are stored. This buffer allows for the algorithm to train on a set of samples at once as is common for deep learning. Next note, the use of an exploration probability. In Q learning, there is a general trade-off between exploration and exploitation. Exploitation refers to choosing the best action using the results seen thus far. The point here is that there is significant value in exploring the action space for a given state, particularly early in the training sequence to help avoid the model getting stuck at a local minimum. Finally, the gradient descent step shows the insight that allows for the application of gradient descent. Let's take a closer look at that:
-<p><img src="/svgs/307d6b9c768ec640ac153455ac54a7a9.svg" align=middle width=204.81905894999997pt height=21.0174195pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/307d6b9c768ec640ac153455ac54a7a9.svg?invert_in_darkmode" align=middle width=204.81905894999997pt height=21.0174195pt/></p>
 and
-<p><img src="/svgs/d7ad40e37c0187c28ed9673e9a38e888.svg" align=middle width=152.7489942pt height=18.905967299999997pt/></p>
+<p align="center"><img src="https://rawgit.com/in	git@github.com:kscharpf/drl_intro/None/svgs/d7ad40e37c0187c28ed9673e9a38e888.svg?invert_in_darkmode" align=middle width=152.7489942pt height=18.905967299999997pt/></p>
 What this says is that we can calculate the *error* in our value function by comparing the value produced by our state-value function for the current state and the action provided versus the actual reward produced by taking that action plus the expected reward from the next state-action pair.
 
 ### DQN Implementation
diff --git a/breakout_5000.pt b/breakout_5000.pt
deleted file mode 100644
index 75cb8d3..0000000
Binary files a/breakout_5000.pt and /dev/null differ
diff --git a/scores2.pkl b/scores2.pkl
deleted file mode 100644
index c1d112e..0000000
Binary files a/scores2.pkl and /dev/null differ
diff --git a/svgs/b60e8a217b0c7ceff66266cf1724b261.svg b/svgs/b60e8a217b0c7ceff66266cf1724b261.svg
new file mode 100644
index 0000000..02b87d8
--- /dev/null
+++ b/svgs/b60e8a217b0c7ceff66266cf1724b261.svg
@@ -0,0 +1,18 @@
+<ns0:svg xmlns:ns0="http://www.w3.org/2000/svg" xmlns:ns1="http://www.w3.org/1999/xlink" height="8.855684pt" readme2tex:offset="0" version="1.1" viewBox="117.235492 -57.000683 45.937555 8.855684" width="45.937555pt" xmlns:readme2tex="http://github.com/leegao/readme2tex/">
+<ns0:defs>
+<ns0:path d="M4.07472 -2.291407H6.854296C6.993773 -2.291407 7.183064 -2.291407 7.183064 -2.49066S6.993773 -2.689913 6.854296 -2.689913H4.07472V-5.479452C4.07472 -5.618929 4.07472 -5.808219 3.875467 -5.808219S3.676214 -5.618929 3.676214 -5.479452V-2.689913H0.886675C0.747198 -2.689913 0.557908 -2.689913 0.557908 -2.49066S0.747198 -2.291407 0.886675 -2.291407H3.676214V0.498132C3.676214 0.637609 3.676214 0.826899 3.875467 0.826899S4.07472 0.637609 4.07472 0.498132V-2.291407Z" id="g1-43" />
+<ns0:path d="M6.844334 -3.257783C6.993773 -3.257783 7.183064 -3.257783 7.183064 -3.457036S6.993773 -3.656289 6.854296 -3.656289H0.886675C0.747198 -3.656289 0.557908 -3.656289 0.557908 -3.457036S0.747198 -3.257783 0.896638 -3.257783H6.844334ZM6.854296 -1.325031C6.993773 -1.325031 7.183064 -1.325031 7.183064 -1.524284S6.993773 -1.723537 6.844334 -1.723537H0.896638C0.747198 -1.723537 0.557908 -1.723537 0.557908 -1.524284S0.747198 -1.325031 0.886675 -1.325031H6.854296Z" id="g1-61" />
+<ns0:path d="M3.716065 -3.765878C3.536737 -4.134496 3.247821 -4.403487 2.799502 -4.403487C1.633873 -4.403487 0.398506 -2.938979 0.398506 -1.484433C0.398506 -0.547945 0.946451 0.109589 1.723537 0.109589C1.92279 0.109589 2.420922 0.069738 3.01868 -0.637609C3.098381 -0.219178 3.447073 0.109589 3.92528 0.109589C4.273973 0.109589 4.503113 -0.119552 4.662516 -0.438356C4.83188 -0.797011 4.961395 -1.404732 4.961395 -1.424658C4.961395 -1.524284 4.871731 -1.524284 4.841843 -1.524284C4.742217 -1.524284 4.732254 -1.484433 4.702366 -1.344956C4.533001 -0.697385 4.353674 -0.109589 3.945205 -0.109589C3.676214 -0.109589 3.646326 -0.368618 3.646326 -0.56787C3.646326 -0.787049 3.666252 -0.86675 3.775841 -1.305106C3.88543 -1.723537 3.905355 -1.823163 3.995019 -2.201743L4.353674 -3.596513C4.423412 -3.875467 4.423412 -3.895392 4.423412 -3.935243C4.423412 -4.104608 4.303861 -4.204234 4.134496 -4.204234C3.895392 -4.204234 3.745953 -3.985056 3.716065 -3.765878ZM3.068493 -1.185554C3.01868 -1.006227 3.01868 -0.986301 2.86924 -0.816936C2.430884 -0.268991 2.022416 -0.109589 1.743462 -0.109589C1.24533 -0.109589 1.105853 -0.657534 1.105853 -1.046077C1.105853 -1.544209 1.424658 -2.769614 1.653798 -3.227895C1.96264 -3.815691 2.410959 -4.184309 2.809465 -4.184309C3.457036 -4.184309 3.596513 -3.367372 3.596513 -3.307597S3.576588 -3.188045 3.566625 -3.138232L3.068493 -1.185554Z" id="g0-97" />
+<ns0:path d="M2.381071 -6.804483C2.381071 -6.814446 2.381071 -6.914072 2.251557 -6.914072C2.022416 -6.914072 1.295143 -6.834371 1.036115 -6.814446C0.956413 -6.804483 0.846824 -6.794521 0.846824 -6.615193C0.846824 -6.495641 0.936488 -6.495641 1.085928 -6.495641C1.564134 -6.495641 1.58406 -6.425903 1.58406 -6.326276C1.58406 -6.256538 1.494396 -5.917808 1.444583 -5.708593L0.627646 -2.460772C0.508095 -1.96264 0.468244 -1.803238 0.468244 -1.454545C0.468244 -0.508095 0.996264 0.109589 1.733499 0.109589C2.909091 0.109589 4.134496 -1.374844 4.134496 -2.809465C4.134496 -3.716065 3.606476 -4.403487 2.809465 -4.403487C2.351183 -4.403487 1.942715 -4.11457 1.643836 -3.805729L2.381071 -6.804483ZM1.444583 -3.038605C1.504359 -3.257783 1.504359 -3.277709 1.594022 -3.387298C2.082192 -4.034869 2.530511 -4.184309 2.789539 -4.184309C3.148194 -4.184309 3.417186 -3.88543 3.417186 -3.247821C3.417186 -2.660025 3.088418 -1.514321 2.909091 -1.135741C2.580324 -0.468244 2.122042 -0.109589 1.733499 -0.109589C1.39477 -0.109589 1.066002 -0.37858 1.066002 -1.115816C1.066002 -1.305106 1.066002 -1.494396 1.225405 -2.122042L1.444583 -3.038605Z" id="g0-98" />
+<ns0:path d="M3.327522 -3.008717C3.387298 -3.267746 3.616438 -4.184309 4.313823 -4.184309C4.363636 -4.184309 4.60274 -4.184309 4.811955 -4.054795C4.533001 -4.004981 4.333748 -3.755915 4.333748 -3.516812C4.333748 -3.35741 4.443337 -3.16812 4.712329 -3.16812C4.931507 -3.16812 5.250311 -3.347447 5.250311 -3.745953C5.250311 -4.26401 4.662516 -4.403487 4.323786 -4.403487C3.745953 -4.403487 3.39726 -3.875467 3.277709 -3.646326C3.028643 -4.303861 2.49066 -4.403487 2.201743 -4.403487C1.165629 -4.403487 0.597758 -3.118306 0.597758 -2.86924C0.597758 -2.769614 0.697385 -2.769614 0.71731 -2.769614C0.797011 -2.769614 0.826899 -2.789539 0.846824 -2.879203C1.185554 -3.935243 1.843088 -4.184309 2.181818 -4.184309C2.371108 -4.184309 2.719801 -4.094645 2.719801 -3.516812C2.719801 -3.20797 2.550436 -2.540473 2.181818 -1.145704C2.022416 -0.52802 1.673724 -0.109589 1.235367 -0.109589C1.175592 -0.109589 0.946451 -0.109589 0.737235 -0.239103C0.986301 -0.288917 1.205479 -0.498132 1.205479 -0.777086C1.205479 -1.046077 0.986301 -1.125778 0.836862 -1.125778C0.537983 -1.125778 0.288917 -0.86675 0.288917 -0.547945C0.288917 -0.089664 0.787049 0.109589 1.225405 0.109589C1.882939 0.109589 2.241594 -0.587796 2.271482 -0.647572C2.391034 -0.278954 2.749689 0.109589 3.347447 0.109589C4.373599 0.109589 4.941469 -1.175592 4.941469 -1.424658C4.941469 -1.524284 4.851806 -1.524284 4.821918 -1.524284C4.732254 -1.524284 4.712329 -1.484433 4.692403 -1.414695C4.363636 -0.348692 3.686177 -0.109589 3.367372 -0.109589C2.978829 -0.109589 2.819427 -0.428394 2.819427 -0.767123C2.819427 -0.986301 2.879203 -1.205479 2.988792 -1.643836L3.327522 -3.008717Z" id="g0-120" />
+<ns0:path d="M4.841843 -3.795766C4.881694 -3.935243 4.881694 -3.955168 4.881694 -4.024907C4.881694 -4.204234 4.742217 -4.293898 4.592777 -4.293898C4.493151 -4.293898 4.333748 -4.234122 4.244085 -4.084682C4.224159 -4.034869 4.144458 -3.726027 4.104608 -3.5467C4.034869 -3.287671 3.965131 -3.01868 3.905355 -2.749689L3.457036 -0.956413C3.417186 -0.806974 2.988792 -0.109589 2.331258 -0.109589C1.823163 -0.109589 1.713574 -0.547945 1.713574 -0.916563C1.713574 -1.374844 1.882939 -1.992528 2.221669 -2.86924C2.381071 -3.277709 2.420922 -3.387298 2.420922 -3.58655C2.420922 -4.034869 2.102117 -4.403487 1.603985 -4.403487C0.657534 -4.403487 0.288917 -2.958904 0.288917 -2.86924C0.288917 -2.769614 0.388543 -2.769614 0.408468 -2.769614C0.508095 -2.769614 0.518057 -2.789539 0.56787 -2.948941C0.836862 -3.88543 1.235367 -4.184309 1.574097 -4.184309C1.653798 -4.184309 1.823163 -4.184309 1.823163 -3.865504C1.823163 -3.616438 1.723537 -3.35741 1.653798 -3.16812C1.255293 -2.11208 1.075965 -1.544209 1.075965 -1.075965C1.075965 -0.18929 1.703611 0.109589 2.291407 0.109589C2.67995 0.109589 3.01868 -0.059776 3.297634 -0.33873C3.16812 0.179328 3.048568 0.667497 2.650062 1.195517C2.391034 1.534247 2.012453 1.823163 1.554172 1.823163C1.414695 1.823163 0.966376 1.793275 0.797011 1.404732C0.956413 1.404732 1.085928 1.404732 1.225405 1.285181C1.325031 1.195517 1.424658 1.066002 1.424658 0.876712C1.424658 0.56787 1.155666 0.52802 1.05604 0.52802C0.826899 0.52802 0.498132 0.687422 0.498132 1.175592C0.498132 1.673724 0.936488 2.042341 1.554172 2.042341C2.580324 2.042341 3.606476 1.135741 3.88543 0.009963L4.841843 -3.795766Z" id="g0-121" />
+</ns0:defs>
+<ns0:g fill-opacity="0.9" id="page1">
+<ns0:use x="117.235492" y="-50.082181" ns1:href="#g0-121" />
+<ns0:use x="125.244768" y="-50.082181" ns1:href="#g1-61" />
+<ns0:use x="135.760848" y="-50.082181" ns1:href="#g0-97" />
+<ns0:use x="141.027002" y="-50.082181" ns1:href="#g0-120" />
+<ns0:use x="148.9348" y="-50.082181" ns1:href="#g1-43" />
+<ns0:use x="158.897413" y="-50.082181" ns1:href="#g0-98" />
+</ns0:g>
+</ns0:svg>
\ No newline at end of file
diff --git a/svgs/tmp.png b/svgs/tmp.png
deleted file mode 100644
index d888b7e..0000000
Binary files a/svgs/tmp.png and /dev/null differ
