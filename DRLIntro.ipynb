{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/75/9e841bc2bc75128e0b65c3d5255d0bd16becb9d8f7120b965d41b8e70041/gym-0.14.0.tar.gz (1.6MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/kevin/anaconda3/envs/torchdrl/lib/python3.7/site-packages (from gym) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/kevin/anaconda3/envs/torchdrl/lib/python3.7/site-packages (from gym) (1.16.5)\n",
      "Requirement already satisfied: six in /home/kevin/anaconda3/envs/torchdrl/lib/python3.7/site-packages (from gym) (1.12.0)\n",
      "Collecting pyglet<=1.3.2,>=1.2.0 (from gym)\n",
      "  Using cached https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl\n",
      "Collecting cloudpickle~=1.2.0 (from gym)\n",
      "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
      "Collecting future (from pyglet<=1.3.2,>=1.2.0->gym)\n",
      "  Using cached https://files.pythonhosted.org/packages/90/52/e20466b85000a181e1e144fd8305caf2cf475e2f9674e797b222f8105f5f/future-0.17.1.tar.gz\n",
      "Building wheels for collected packages: gym, future\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.14.0-cp37-none-any.whl size=1637526 sha256=8ec89bed012210e268189bef254a1fa86d1aa187685a5add8acc5fac19b8354d\n",
      "  Stored in directory: /home/kevin/.cache/pip/wheels/7e/53/f6/c0cd3c9bf953f35c0aee7fa62ea209371e92f5e5cced3245ba\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.17.1-cp37-none-any.whl size=488730 sha256=fdf5aea76a1ef8ecae79760f976b9c55d13dddb08cb2f4a199ced16471ab9f06\n",
      "  Stored in directory: /home/kevin/.cache/pip/wheels/0c/61/d2/d6b7317325828fbb39ee6ad559dbe4664d0896da4721bf379e\n",
      "Successfully built gym future\n",
      "Installing collected packages: future, pyglet, cloudpickle, gym\n",
      "Successfully installed cloudpickle-1.2.2 future-0.17.1 gym-0.14.0 pyglet-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 500\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matplotlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1218fea4a243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# set up matplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mis_ipython\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'inline'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_ipython\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'matplotlib' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gym\n",
    "try:\n",
    "    env\n",
    "except: \n",
    "    env = None\n",
    "if env:\n",
    "    env.close()\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env.reset()\n",
    "last_screen = get_screen()\n",
    "current_screen = get_screen()\n",
    "state = current_screen - last_screen\n",
    "for i in range(300):\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    # Select and perform an action\n",
    "    action = select_action(state)\n",
    "    _, reward, done, _ = env.step(action.item())\n",
    "    \n",
    "    print(\"i: {} reward {}\".format(i, reward))\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "\n",
    "    # Observe new state\n",
    "    last_screen = current_screen\n",
    "    current_screen = get_screen()\n",
    "    \n",
    "    if done:\n",
    "        print(\"DONE\")\n",
    "        break\n",
    "    next_state = current_screen - last_screen\n",
    "\n",
    "    # Move to the next state\n",
    "    state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Images of type float must be between -1 and 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f014bd131009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astronaut.png'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mastronaut_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astronaut_gray.png'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgray_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astronaut_edges.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, plugin, check_contrast, **plugin_args)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s is a boolean image: setting True to 1 and False to 0'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imsave'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m                                (plugin, kind))\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, format_str, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid number of channels in image array.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndarray_to_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\u001b[0m in \u001b[0;36mndarray_to_pil\u001b[0;34m(arr, format_str)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'f'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_as_uint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/util/dtype.py\u001b[0m in \u001b[0;36mimg_as_uint\u001b[0;34m(image, force_copy)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[0;32m--> 437\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/util/dtype.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(image, dtype, force_copy, uniform)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Images of type float must be between -1 and 1.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;31m# floating point -> integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mprec_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Images of type float must be between -1 and 1."
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "from skimage.viewer import ImageViewer\n",
    "from skimage import data, color, io\n",
    "import numpy as np\n",
    "\n",
    "# suppress warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "\n",
    "# convolution code derived from \n",
    "# http://machinelearninguru.com/computer_vision/basics/convolution/image_convolution_1.html\n",
    "def convolve(img, kernel):    \n",
    "    \"\"\"assume 3x3 kernel shape\"\"\"\n",
    "    # construct an output array of the same size as the input image\n",
    "    output = np.zeros_like(img)\n",
    "    \n",
    "    # create a padded image so that we can convolve through the whole thing, \"same\" padding\n",
    "    image_padded = np.zeros((img.shape[0] + kernel.shape[0]-1, img.shape[1]+kernel.shape[1]-1))\n",
    "    image_padded[1:-1,1:-1] = img\n",
    "    for x in range(img.shape[1]):\n",
    "        for y in range(img.shape[0]):\n",
    "            output[y,x] = (kernel * image_padded[y:y+3,x:x+3]).sum()\n",
    "    return output\n",
    "\n",
    "def preprocess(img):\n",
    "    return color.rgb2gray(img)\n",
    "\n",
    "astronaut_image = data.astronaut()\n",
    "gray_img = preprocess(astronaut_image)\n",
    "# edge detection kernel: \n",
    "edge_img = convolve(gray_img, np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]]))\n",
    "\n",
    "io.imsave('astronaut.png',astronaut_image)\n",
    "io.imsave('astronaut_gray.png',gray_img)\n",
    "io.imsave('astronaut_edges.png', edge_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = np.array([[1,0],[0,1]])\n",
    "kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-13d1ec0cb0db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 1 with size 3"
     ]
    }
   ],
   "source": [
    "O = np.zeros_like(img)\n",
    "m = 0\n",
    "n = 0\n",
    "for m in range(img.shape[0]):\n",
    "    for n in range(img.shape[1]):\n",
    "        for i in range(kernel.shape[0]):\n",
    "            for j in range(kernel.shape[1]):\n",
    "                O[i,j] += kernel[i,j] * img[i+m,j+n]\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1],\n",
       "       [-1,  8, -1],\n",
       "       [-1, -1, -1]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_filter = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])\n",
    "edge_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = np.array([[1,1,1],[1,1,1],[1,1,1]])\n",
    "(I * edge_filter).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_edge_filter = np.flipud(np.fliplr(right_edge_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.zeros_like(astronaut_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512.0\n"
     ]
    }
   ],
   "source": [
    "def conv_output_size(input_dim, padding, stride, kernel_dim):\n",
    "    P = 1 if padding == 'same' else 0\n",
    "    return (input_dim - kernel_dim + P*2)/stride + 1\n",
    "print(conv_output_size(512,'same',1,3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import color\n",
    "gray_image = color.rgb2gray(astronaut_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'astronaut_gray.bmp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-3f3b0d2b4a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgray_image2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astronaut_gray.bmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray_image2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, flatten, **plugin_args)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m                                (plugin, kind))\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, dtype, img_num, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \"\"\"\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpil_to_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'astronaut_gray.bmp'"
     ]
    }
   ],
   "source": [
    "gray_image2 = io.imread('astronaut_gray.bmp')\n",
    "viewer = ImageViewer(gray_image2)\n",
    "viewer.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_padded = np.zeros((gray_image.shape[0] + 2, gray_image.shape[1]+2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_padded[1:-1,1:-1] = gray_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/anaconda3/envs/drlnd/lib/python3.6/site-packages/skimage/io/_io.py:141: UserWarning: astronaut_edges.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n"
     ]
    }
   ],
   "source": [
    "for x in range(gray_image.shape[1]):\n",
    "    for y in range(gray_image.shape[0]):\n",
    "        output[y,x] = (right_edge_filter * image_padded[y:y+3,x:x+3]).sum()\n",
    "        \n",
    "io.imsave('astronaut_edges.png', output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = ImageViewer(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3ccc102dc542>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageViewer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "from skimage.viewer import ImageViewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs, seed):\n",
    "        super(DQN, self).__init__()\n",
    "        print(\"h: {} w: {} outputs {}\".format(h, w, outputs))\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(w,kernel_size=8,stride=4), kernel_size=4, stride=2)\n",
    "        convh = conv2d_size_out(conv2d_size_out(h, kernel_size=8, stride=4), \n",
    "                                                kernel_size=4, stride=2)\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.fc1 = nn.Linear(linear_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, outputs)\n",
    "        \n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = int(1e5)\n",
    "TAU = 0.01\n",
    "GAMMA = 0.99\n",
    "LR=5e-4\n",
    "class DQNAgent:\n",
    "    def __init__(self, height, width, action_size, seed):\n",
    "        self.seed = random.seed(seed)\n",
    "        self.action_size = action_size\n",
    "        self.batch_indices = torch.arange(BATCH_SIZE).long().to(device)\n",
    "        self.samples_before_learning = 1000\n",
    "        self.learn_interval = 20\n",
    "        self.parameter_update_interval = 2\n",
    "        self.tau = TAU\n",
    "        self.gamma = GAMMA\n",
    "\n",
    "        self.qnetwork_local = DQN(height, width, action_size, seed).to(device)\n",
    "        self.qnetwork_target = DQN(height, width, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def act(self, state, eps=0.):\n",
    "    \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        state = state.reshape((1,state.shape[0], state.shape[1], state.shape[2]))\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        if random.random() < eps:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        else:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        state = state.reshape((1,1,state.shape[0], state.shape[1]))\n",
    "        next_state = next_state.reshape((1,1,next_state.shape[0], next_state.shape[1]))\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.t_step += 1\n",
    "        if self.t_step % self.learn_interval == 0:\n",
    "            if len(self.memory) > self.samples_before_learning:\n",
    "                #state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "                state = torch.from_numpy(state).float().to(device)\n",
    "\n",
    "                #next_state = torch.from_numpy(next_state).float().unsqueeze(0).to(device)\n",
    "                next_state = torch.from_numpy(next_state).float().to(device)\n",
    "\n",
    "                target = self.qnetwork_local(state).data\n",
    "                old_val = target[0][action]\n",
    "                target_val = self.qnetwork_target(next_state).data\n",
    "                if done:\n",
    "                    target[0][action] = reward\n",
    "                else:\n",
    "                    target[0][action] = reward + self.gamma * torch.max(target_val)\n",
    "                indices=None\n",
    "                weights=None\n",
    "                states, actions, rewards, next_states, dones = self.memory.sample()\n",
    "\n",
    "                self.learn(states, actions, rewards, next_states, dones, indices, weights, self.gamma)\n",
    "        \n",
    "    def learn(self, states, actions, rewards, next_states, dones, indices, weights, gamma):\n",
    "        states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack(dones)).float().to(device)\n",
    "        states = states.reshape((BATCH_SIZE, 1, states.shape[1], states.shape[2]))\n",
    "        next_states = next_states.reshape((BATCH_SIZE, 1, next_states.shape[1], next_states.shape[2]))\n",
    "\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach()\n",
    "\n",
    "        Q_targets_next = Q_targets_next.max(1)[0]\n",
    "\n",
    "        Q_targets = rewards + gamma * Q_targets_next.reshape((BATCH_SIZE, 1)) * (1 - dones)\n",
    "\n",
    "        pred = self.qnetwork_local(states)\n",
    "        Q_expected = pred.gather(1, actions)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.t_step % self.parameter_update_interval == 0:\n",
    "            self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
    "\n",
    "    def soft_update(self, qnetwork_local, qnetwork_target, tau):\n",
    "        for local_param, target_param in zip(qnetwork_local.parameters(), qnetwork_target.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grayscale(img):\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    return to_grayscale(downsample(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: 105 w: 80 outputs 4\n",
      "h: 105 w: 80 outputs 4\n",
      "Game   0 Score 1.000 completed in   175 steps eps 0.991 last 10 avg 0.000\n",
      "Game  10 Score 0.000 completed in   129 steps eps 0.897 last 10 avg 0.000\n",
      "Game  20 Score 0.000 completed in   131 steps eps 0.817 last 10 avg 1.091\n",
      "Game  30 Score 1.000 completed in   158 steps eps 0.749 last 10 avg 1.000\n",
      "Game  40 Score 0.000 completed in   140 steps eps 0.685 last 10 avg 1.000\n",
      "Game  50 Score 7.000 completed in   401 steps eps 0.615 last 10 avg 1.909\n",
      "Game  60 Score 0.000 completed in   154 steps eps 0.563 last 10 avg 1.455\n",
      "Game  70 Score 0.000 completed in   124 steps eps 0.518 last 10 avg 0.727\n",
      "Game  80 Score 0.000 completed in   153 steps eps 0.480 last 10 avg 0.545\n",
      "Game  90 Score 0.000 completed in   145 steps eps 0.443 last 10 avg 0.455\n",
      "Game 100 Score 1.000 completed in   225 steps eps 0.404 last 10 avg 1.182\n",
      "Game 110 Score 1.000 completed in   195 steps eps 0.371 last 10 avg 0.909\n",
      "Game 120 Score 2.000 completed in   201 steps eps 0.340 last 10 avg 1.091\n",
      "Game 130 Score 3.000 completed in   232 steps eps 0.310 last 10 avg 1.545\n",
      "Game 140 Score 2.000 completed in   223 steps eps 0.282 last 10 avg 1.636\n",
      "Game 150 Score 1.000 completed in   173 steps eps 0.256 last 10 avg 1.455\n",
      "Game 160 Score 1.000 completed in   176 steps eps 0.234 last 10 avg 1.273\n",
      "Game 170 Score 3.000 completed in   213 steps eps 0.213 last 10 avg 1.455\n",
      "Game 180 Score 0.000 completed in   122 steps eps 0.196 last 10 avg 1.364\n",
      "Game 190 Score 2.000 completed in   244 steps eps 0.177 last 10 avg 1.545\n",
      "Game 200 Score 0.000 completed in   123 steps eps 0.164 last 10 avg 1.364\n",
      "Game 210 Score 2.000 completed in   205 steps eps 0.150 last 10 avg 1.182\n",
      "Game 220 Score 2.000 completed in   275 steps eps 0.135 last 10 avg 1.636\n",
      "Game 230 Score 1.000 completed in   179 steps eps 0.123 last 10 avg 1.636\n",
      "Game 240 Score 0.000 completed in   139 steps eps 0.113 last 10 avg 1.182\n",
      "Game 250 Score 0.000 completed in   189 steps eps 0.100 last 10 avg 1.909\n",
      "Game 260 Score 1.000 completed in   191 steps eps 0.100 last 10 avg 1.545\n",
      "Game 270 Score 5.000 completed in   418 steps eps 0.100 last 10 avg 2.727\n",
      "Game 280 Score 3.000 completed in   210 steps eps 0.100 last 10 avg 1.636\n",
      "Game 290 Score 2.000 completed in   200 steps eps 0.100 last 10 avg 3.273\n",
      "Game 300 Score 3.000 completed in   232 steps eps 0.100 last 10 avg 3.0914\n",
      "Game 310 Score 2.000 completed in   194 steps eps 0.100 last 10 avg 2.455\n",
      "Game 320 Score 3.000 completed in   247 steps eps 0.100 last 10 avg 2.364\n",
      "Game 330 Score 4.000 completed in   309 steps eps 0.100 last 10 avg 2.000\n",
      "Game 340 Score 4.000 completed in   290 steps eps 0.100 last 10 avg 2.545\n",
      "Game 350 Score 3.000 completed in   269 steps eps 0.100 last 10 avg 2.909\n",
      "Game 360 Score 5.000 completed in   311 steps eps 0.100 last 10 avg 2.818\n",
      "Game 370 Score 3.000 completed in   213 steps eps 0.100 last 10 avg 3.000\n",
      "Game 380 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 2.545\n",
      "Game 390 Score 3.000 completed in   224 steps eps 0.100 last 10 avg 2.909\n",
      "Game 400 Score 2.000 completed in   198 steps eps 0.100 last 10 avg 3.182\n",
      "Game 410 Score 2.000 completed in   194 steps eps 0.100 last 10 avg 3.636\n",
      "Game 420 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 3.545\n",
      "Game 430 Score 1.000 completed in   170 steps eps 0.100 last 10 avg 2.545\n",
      "Game 440 Score 3.000 completed in   235 steps eps 0.100 last 10 avg 2.818\n",
      "Game 450 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 3.273\n",
      "Game 460 Score 1.000 completed in   150 steps eps 0.100 last 10 avg 3.000\n",
      "Game 470 Score 3.000 completed in   213 steps eps 0.100 last 10 avg 3.091\n",
      "Game 480 Score 3.000 completed in   235 steps eps 0.100 last 10 avg 3.182\n",
      "Game 490 Score 3.000 completed in   213 steps eps 0.100 last 10 avg 2.545\n",
      "Game 500 Score 3.000 completed in   303 steps eps 0.100 last 10 avg 3.727\n",
      "Game 510 Score 3.000 completed in   213 steps eps 0.100 last 10 avg 3.273\n",
      "Game 520 Score 4.000 completed in   289 steps eps 0.100 last 10 avg 3.455\n",
      "Game 530 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 3.000\n",
      "Game 540 Score 4.000 completed in   258 steps eps 0.100 last 10 avg 3.636\n",
      "Game 550 Score 1.000 completed in   161 steps eps 0.100 last 10 avg 3.091\n",
      "Game 560 Score 3.000 completed in   248 steps eps 0.100 last 10 avg 3.545\n",
      "Game 570 Score 3.000 completed in   223 steps eps 0.100 last 10 avg 3.636\n",
      "Game 580 Score 4.000 completed in   243 steps eps 0.100 last 10 avg 3.909\n",
      "Game 590 Score 2.000 completed in   179 steps eps 0.100 last 10 avg 3.636\n",
      "Game 600 Score 3.000 completed in   218 steps eps 0.100 last 10 avg 3.636\n",
      "Game 610 Score 7.000 completed in   404 steps eps 0.100 last 10 avg 3.182\n",
      "Game 620 Score 2.000 completed in   179 steps eps 0.100 last 10 avg 3.909\n",
      "Game 630 Score 5.000 completed in   271 steps eps 0.100 last 10 avg 3.545\n",
      "Game 640 Score 6.000 completed in   328 steps eps 0.100 last 10 avg 4.182\n",
      "Game 650 Score 5.000 completed in   286 steps eps 0.100 last 10 avg 4.455\n",
      "Game 660 Score 3.000 completed in   212 steps eps 0.100 last 10 avg 4.5455\n",
      "Game 670 Score 2.000 completed in   231 steps eps 0.100 last 10 avg 3.9092\n",
      "Game 680 Score 2.000 completed in   236 steps eps 0.100 last 10 avg 4.7273\n",
      "Game 690 Score 2.000 completed in   186 steps eps 0.100 last 10 avg 3.727\n",
      "Game 700 Score 3.000 completed in   317 steps eps 0.100 last 10 avg 3.636\n",
      "Game 710 Score 4.000 completed in   256 steps eps 0.100 last 10 avg 4.091\n",
      "Game 720 Score 5.000 completed in   357 steps eps 0.100 last 10 avg 4.727\n",
      "Game 730 Score 5.000 completed in   290 steps eps 0.100 last 10 avg 5.000\n",
      "Game 740 Score 4.000 completed in   259 steps eps 0.100 last 10 avg 4.3643\n",
      "Game 750 Score 6.000 completed in   315 steps eps 0.100 last 10 avg 5.091\n",
      "Game 760 Score 4.000 completed in   301 steps eps 0.100 last 10 avg 5.7275\n",
      "Game 770 Score 2.000 completed in   203 steps eps 0.100 last 10 avg 5.273\n",
      "Game 780 Score 5.000 completed in   313 steps eps 0.100 last 10 avg 3.545\n",
      "Game 790 Score 6.000 completed in   348 steps eps 0.100 last 10 avg 4.727\n",
      "Game 800 Score 4.000 completed in   283 steps eps 0.100 last 10 avg 5.000\n",
      "Game 810 Score 5.000 completed in   307 steps eps 0.100 last 10 avg 4.545\n",
      "Game 820 Score 5.000 completed in   293 steps eps 0.100 last 10 avg 5.9098\n",
      "Game 830 Score 2.000 completed in   182 steps eps 0.100 last 10 avg 4.818\n",
      "Game 840 Score 2.000 completed in   199 steps eps 0.100 last 10 avg 3.818\n",
      "Game 850 Score 2.000 completed in   293 steps eps 0.100 last 10 avg 4.7279\n",
      "Game 860 Score 4.000 completed in   256 steps eps 0.100 last 10 avg 4.182\n",
      "Game 870 Score 4.000 completed in   331 steps eps 0.100 last 10 avg 5.0009\n",
      "Game 880 Score 6.000 completed in   428 steps eps 0.100 last 10 avg 4.636\n",
      "Game 890 Score 8.000 completed in   305 steps eps 0.100 last 10 avg 5.364\n",
      "Game 900 Score 9.000 completed in   421 steps eps 0.100 last 10 avg 4.364\n",
      "Game 910 Score 4.000 completed in   274 steps eps 0.100 last 10 avg 5.182\n",
      "Game 920 Score 6.000 completed in   334 steps eps 0.100 last 10 avg 4.000\n",
      "Game 930 Score 6.000 completed in   417 steps eps 0.100 last 10 avg 4.909\n",
      "Game 940 Score 2.000 completed in   220 steps eps 0.100 last 10 avg 4.182\n",
      "Game 950 Score 6.000 completed in   341 steps eps 0.100 last 10 avg 4.364\n",
      "Game 960 Score 11.000 completed in   430 steps eps 0.100 last 10 avg 6.364\n",
      "Game 970 Score 6.000 completed in   348 steps eps 0.100 last 10 avg 6.182\n",
      "Game 980 Score 7.000 completed in   374 steps eps 0.100 last 10 avg 6.1825\n",
      "Game 990 Score 6.000 completed in   365 steps eps 0.100 last 10 avg 5.364\n",
      "Game 1000 Score 4.000 completed in   315 steps eps 0.100 last 10 avg 4.818\n",
      "Game 1010 Score 5.000 completed in   286 steps eps 0.100 last 10 avg 6.3648\n",
      "Game 1020 Score 6.000 completed in   361 steps eps 0.100 last 10 avg 4.818\n",
      "Game 1030 Score 6.000 completed in   476 steps eps 0.100 last 10 avg 5.182\n",
      "Game 1040 Score 6.000 completed in   343 steps eps 0.100 last 10 avg 4.818\n",
      "Game 1050 Score 5.000 completed in   302 steps eps 0.100 last 10 avg 4.7278\n",
      "Game 1060 Score 4.000 completed in   296 steps eps 0.100 last 10 avg 4.455\n",
      "Game 1070 Score 2.000 completed in   316 steps eps 0.100 last 10 avg 5.364\n",
      "Game 1080 Score 4.000 completed in   275 steps eps 0.100 last 10 avg 5.455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1090 Score 7.000 completed in   360 steps eps 0.100 last 10 avg 4.9092\n",
      "Game 1100 Score 2.000 completed in   182 steps eps 0.100 last 10 avg 5.000\n",
      "Game 1110 Score 9.000 completed in   448 steps eps 0.100 last 10 avg 3.909\n",
      "Game 1120 Score 5.000 completed in   365 steps eps 0.100 last 10 avg 3.455\n",
      "Game 1130 Score 2.000 completed in   220 steps eps 0.100 last 10 avg 4.364\n",
      "Game 1140 Score 4.000 completed in   268 steps eps 0.100 last 10 avg 4.636\n",
      "Game 1150 Score 3.000 completed in   250 steps eps 0.100 last 10 avg 4.545\n",
      "Game 1160 Score 7.000 completed in   398 steps eps 0.100 last 10 avg 4.364\n",
      "Game 1170 Score 7.000 completed in   411 steps eps 0.100 last 10 avg 4.909\n",
      "Game 1180 Score 9.000 completed in   425 steps eps 0.100 last 10 avg 5.818\n",
      "Game 1190 Score 3.000 completed in   295 steps eps 0.100 last 10 avg 5.545\n",
      "Game 1200 Score 4.000 completed in   292 steps eps 0.100 last 10 avg 3.909\n",
      "Game 1210 Score 5.000 completed in   379 steps eps 0.100 last 10 avg 5.0916\n",
      "Game 1220 Score 7.000 completed in   265 steps eps 0.100 last 10 avg 5.273\n",
      "Game 1230 Score 2.000 completed in   183 steps eps 0.100 last 10 avg 4.091\n",
      "Game 1240 Score 3.000 completed in   267 steps eps 0.100 last 10 avg 5.2734\n",
      "Game 1250 Score 2.000 completed in   275 steps eps 0.100 last 10 avg 3.909\n",
      "Game 1260 Score 8.000 completed in   442 steps eps 0.100 last 10 avg 6.5457\n",
      "Game 1270 Score 7.000 completed in   388 steps eps 0.100 last 10 avg 5.273\n",
      "Game 1280 Score 4.000 completed in   290 steps eps 0.100 last 10 avg 5.8183\n",
      "Game 1290 Score 3.000 completed in   301 steps eps 0.100 last 10 avg 4.727\n",
      "Game 1300 Score 3.000 completed in   277 steps eps 0.100 last 10 avg 4.4556\n",
      "Game 1310 Score 7.000 completed in   281 steps eps 0.100 last 10 avg 4.6365\n",
      "Game 1320 Score 2.000 completed in   372 steps eps 0.100 last 10 avg 5.727\n",
      "Game 1330 Score 4.000 completed in   267 steps eps 0.100 last 10 avg 4.636\n",
      "Game 1340 Score 4.000 completed in   257 steps eps 0.100 last 10 avg 4.364\n",
      "Game 1350 Score 4.000 completed in   238 steps eps 0.100 last 10 avg 3.727\n",
      "Game 1360 Score 2.000 completed in   183 steps eps 0.100 last 10 avg 6.000\n",
      "Game 1370 Score 2.000 completed in   200 steps eps 0.100 last 10 avg 3.818\n",
      "Game 1380 Score 1.000 completed in   150 steps eps 0.100 last 10 avg 5.4554\n",
      "Game 1390 Score 6.000 completed in   347 steps eps 0.100 last 10 avg 4.545\n",
      "Game 1400 Score 8.000 completed in   377 steps eps 0.100 last 10 avg 6.7277\n",
      "Game 1410 Score 4.000 completed in   289 steps eps 0.100 last 10 avg 5.091\n",
      "Game 1420 Score 5.000 completed in   355 steps eps 0.100 last 10 avg 4.273\n",
      "Game 1430 Score 5.000 completed in   393 steps eps 0.100 last 10 avg 5.364\n",
      "Game 1440 Score 7.000 completed in   314 steps eps 0.100 last 10 avg 5.5454\n",
      "Game 1450 Score 4.000 completed in   340 steps eps 0.100 last 10 avg 5.091\n",
      "Game 1460 Score 7.000 completed in   486 steps eps 0.100 last 10 avg 5.7276\n",
      "Game 1470 Score 3.000 completed in   228 steps eps 0.100 last 10 avg 4.909\n",
      "Game 1480 Score 5.000 completed in   287 steps eps 0.100 last 10 avg 4.455\n",
      "Game 1490 Score 7.000 completed in   455 steps eps 0.100 last 10 avg 5.455\n",
      "Game 1500 Score 3.000 completed in   266 steps eps 0.100 last 10 avg 4.455\n",
      "Game 1510 Score 6.000 completed in   376 steps eps 0.100 last 10 avg 4.909\n",
      "Game 1520 Score 5.000 completed in   393 steps eps 0.100 last 10 avg 5.000\n",
      "Game 1530 Score 4.000 completed in   342 steps eps 0.100 last 10 avg 5.091\n",
      "Game 1540 Score 9.000 completed in   362 steps eps 0.100 last 10 avg 4.727\n",
      "Game 1550 Score 7.000 completed in   382 steps eps 0.100 last 10 avg 4.818\n",
      "Game 1560 Score 4.000 completed in   250 steps eps 0.100 last 10 avg 4.818\n",
      "Game 1570 Score 5.000 completed in   357 steps eps 0.100 last 10 avg 4.455\n",
      "Game 1580 Score 8.000 completed in   370 steps eps 0.100 last 10 avg 4.4551\n",
      "Game 1590 Score 4.000 completed in   376 steps eps 0.100 last 10 avg 5.8185\n",
      "Game 1600 Score 4.000 completed in   291 steps eps 0.100 last 10 avg 4.000\n",
      "Game 1610 Score 7.000 completed in   263 steps eps 0.100 last 10 avg 5.7275\n",
      "Game 1620 Score 8.000 completed in   402 steps eps 0.100 last 10 avg 5.000\n",
      "Game 1630 Score 3.000 completed in   227 steps eps 0.100 last 10 avg 5.9092\n",
      "Game 1640 Score 4.000 completed in   262 steps eps 0.100 last 10 avg 4.182\n",
      "Game 1650 Score 3.000 completed in   333 steps eps 0.100 last 10 avg 4.091\n",
      "Game 1660 Score 3.000 completed in   228 steps eps 0.100 last 10 avg 4.182\n",
      "Game 1670 Score 4.000 completed in   473 steps eps 0.100 last 10 avg 4.364\n",
      "Game 1680 Score 3.000 completed in   314 steps eps 0.100 last 10 avg 4.182\n",
      "Game 1690 Score 7.000 completed in   285 steps eps 0.100 last 10 avg 4.364\n",
      "Game 1700 Score 5.000 completed in   336 steps eps 0.100 last 10 avg 5.091\n",
      "Game 1710 Score 6.000 completed in   423 steps eps 0.100 last 10 avg 4.636\n",
      "Game 1720 Score 6.000 completed in   350 steps eps 0.100 last 10 avg 6.4553\n",
      "Game 1730 Score 7.000 completed in   257 steps eps 0.100 last 10 avg 5.818\n",
      "Game 1740 Score 4.000 completed in   265 steps eps 0.100 last 10 avg 5.4550\n",
      "Game 1750 Score 4.000 completed in   300 steps eps 0.100 last 10 avg 5.000\n",
      "Game 1760 Score 7.000 completed in   455 steps eps 0.100 last 10 avg 5.6368\n",
      "Game 1770 Score 4.000 completed in   354 steps eps 0.100 last 10 avg 4.727\n",
      "Game 1780 Score 5.000 completed in   295 steps eps 0.100 last 10 avg 5.273\n",
      "Game 1790 Score 7.000 completed in   343 steps eps 0.100 last 10 avg 4.8185\n",
      "Game 1800 Score 6.000 completed in   390 steps eps 0.100 last 10 avg 5.545\n",
      "Game 1810 Score 4.000 completed in   287 steps eps 0.100 last 10 avg 4.273\n",
      "Game 1820 Score 4.000 completed in   295 steps eps 0.100 last 10 avg 4.364\n",
      "Game 1830 Score 3.000 completed in   209 steps eps 0.100 last 10 avg 3.909\n",
      "Game 1840 Score 4.000 completed in   309 steps eps 0.100 last 10 avg 5.000\n",
      "Game 1850 Score 7.000 completed in   422 steps eps 0.100 last 10 avg 5.7273\n",
      "Game 1860 Score 8.000 completed in   388 steps eps 0.100 last 10 avg 6.0005\n",
      "Game 1870 Score 3.000 completed in   310 steps eps 0.100 last 10 avg 4.364\n",
      "Game 1880 Score 3.000 completed in   237 steps eps 0.100 last 10 avg 4.545\n",
      "Game 1890 Score 4.000 completed in   255 steps eps 0.100 last 10 avg 5.3641\n",
      "Game 1900 Score 4.000 completed in   266 steps eps 0.100 last 10 avg 5.4556\n",
      "Game 1910 Score 3.000 completed in   311 steps eps 0.100 last 10 avg 3.455\n",
      "Game 1920 Score 9.000 completed in   435 steps eps 0.100 last 10 avg 4.909\n",
      "Game 1930 Score 4.000 completed in   254 steps eps 0.100 last 10 avg 5.182\n",
      "Game 1940 Score 3.000 completed in   374 steps eps 0.100 last 10 avg 4.636\n",
      "Game 1950 Score 7.000 completed in   253 steps eps 0.100 last 10 avg 5.6367\n",
      "Game 1960 Score 3.000 completed in   226 steps eps 0.100 last 10 avg 4.727\n",
      "Game 1970 Score 6.000 completed in   344 steps eps 0.100 last 10 avg 5.7275\n",
      "Game 1980 Score 10.000 completed in   609 steps eps 0.100 last 10 avg 6.545\n",
      "Game 1990 Score 6.000 completed in   351 steps eps 0.100 last 10 avg 4.455\n",
      "Game 2000 Score 1.000 completed in   168 steps eps 0.100 last 10 avg 4.4558\n",
      "Game 2010 Score 3.000 completed in   291 steps eps 0.100 last 10 avg 3.091\n",
      "Game 2020 Score 5.000 completed in   358 steps eps 0.100 last 10 avg 4.727\n",
      "Game 2030 Score 4.000 completed in   298 steps eps 0.100 last 10 avg 4.091\n",
      "Game 2040 Score 5.000 completed in   403 steps eps 0.100 last 10 avg 5.3648\n",
      "Game 2050 Score 6.000 completed in   330 steps eps 0.100 last 10 avg 5.6362\n",
      "Game 2060 Score 6.000 completed in   321 steps eps 0.100 last 10 avg 5.091\n",
      "Game 2070 Score 11.000 completed in   402 steps eps 0.100 last 10 avg 6.000\n",
      "Game 2080 Score 4.000 completed in   279 steps eps 0.100 last 10 avg 5.5452\n",
      "Game 2090 Score 3.000 completed in   225 steps eps 0.100 last 10 avg 3.818\n",
      "Game 2100 Score 3.000 completed in   242 steps eps 0.100 last 10 avg 4.6361\n",
      "Game 2110 Score 5.000 completed in   341 steps eps 0.100 last 10 avg 4.182\n",
      "Game 2120 Score 7.000 completed in   328 steps eps 0.100 last 10 avg 4.545\n",
      "Game 2130 Score 5.000 completed in   285 steps eps 0.100 last 10 avg 6.0002\n",
      "Game 2140 Score 7.000 completed in   443 steps eps 0.100 last 10 avg 4.727\n",
      "Game 2150 Score 4.000 completed in   351 steps eps 0.100 last 10 avg 5.364\n",
      "Game 2160 Score 5.000 completed in   417 steps eps 0.100 last 10 avg 6.0007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 2170 Score 4.000 completed in   333 steps eps 0.100 last 10 avg 5.182\n",
      "Game 2180 Score 7.000 completed in   418 steps eps 0.100 last 10 avg 4.909\n",
      "Game 2190 Score 8.000 completed in   295 steps eps 0.100 last 10 avg 5.9092\n",
      "Game 2200 Score 3.000 completed in   214 steps eps 0.100 last 10 avg 4.818\n",
      "Game 2210 Score 4.000 completed in   238 steps eps 0.100 last 10 avg 3.909\n",
      "Game 2220 Score 8.000 completed in   412 steps eps 0.100 last 10 avg 6.8185\n",
      "Game 2230 Score 4.000 completed in   366 steps eps 0.100 last 10 avg 4.727\n",
      "Game 2240 Score 5.000 completed in   307 steps eps 0.100 last 10 avg 4.273\n",
      "Game 2250 Score 2.000 completed in   251 steps eps 0.100 last 10 avg 2.818\n",
      "Game 2260 Score 4.000 completed in   330 steps eps 0.100 last 10 avg 3.545\n",
      "Game 2270 Score 3.000 completed in   395 steps eps 0.100 last 10 avg 6.1821\n",
      "Game 2280 Score 4.000 completed in   260 steps eps 0.100 last 10 avg 5.091\n",
      "Game 2290 Score 4.000 completed in   319 steps eps 0.100 last 10 avg 3.727\n",
      "Game 2300 Score 5.000 completed in   320 steps eps 0.100 last 10 avg 4.0000\n",
      "Game 2310 Score 3.000 completed in   240 steps eps 0.100 last 10 avg 6.3649\n",
      "Game 2320 Score 12.000 completed in   351 steps eps 0.100 last 10 avg 6.182\n",
      "Game 2330 Score 4.000 completed in   369 steps eps 0.100 last 10 avg 6.1829\n",
      "Game 2340 Score 3.000 completed in   253 steps eps 0.100 last 10 avg 4.000\n",
      "Game 2350 Score 3.000 completed in   448 steps eps 0.100 last 10 avg 3.182\n",
      "Game 2360 Score 9.000 completed in   438 steps eps 0.100 last 10 avg 5.4555\n",
      "Game 2370 Score 5.000 completed in   299 steps eps 0.100 last 10 avg 3.909\n",
      "Game 2380 Score 2.000 completed in   384 steps eps 0.100 last 10 avg 4.545\n",
      "Game 2390 Score 7.000 completed in   258 steps eps 0.100 last 10 avg 5.545\n",
      "Game 2400 Score 2.000 completed in   253 steps eps 0.100 last 10 avg 4.000\n",
      "Game 2410 Score 3.000 completed in   266 steps eps 0.100 last 10 avg 4.636\n",
      "Game 2420 Score 13.000 completed in   336 steps eps 0.100 last 10 avg 5.545\n",
      "Game 2430 Score 4.000 completed in   297 steps eps 0.100 last 10 avg 5.727\n",
      "Game 2440 Score 9.000 completed in   313 steps eps 0.100 last 10 avg 5.000\n",
      "Game 2450 Score 3.000 completed in   293 steps eps 0.100 last 10 avg 6.0007\n",
      "Game 2460 Score 3.000 completed in   331 steps eps 0.100 last 10 avg 4.3645\n",
      "Game 2470 Score 12.000 completed in   322 steps eps 0.100 last 10 avg 5.364\n",
      "Game 2480 Score 4.000 completed in   286 steps eps 0.100 last 10 avg 5.091\n",
      "Game 2490 Score 7.000 completed in   279 steps eps 0.100 last 10 avg 4.636\n",
      "Game 2499 Score 11.000 completed in   492 steps eps 0.100 last 10 avg 5.273"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "action_size = env.action_space.n\n",
    "state = env.reset()\n",
    "height, width = preprocess(state).shape\n",
    "EPS_DECAY  = 0.99995\n",
    "EPS_MIN = 0.1\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_episodes = 2500\n",
    "agent = DQNAgent(height, width, action_size, 0)\n",
    "\n",
    "scores = []\n",
    "eps = 1.0\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "    state = preprocess(state)\n",
    "    score = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # Select and perform an action\n",
    "        action = agent.act(state, eps)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        next_state = preprocess(next_state)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        \n",
    "        score += reward.cpu()[0]\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        eps *= EPS_DECAY\n",
    "        eps = max(eps, EPS_MIN)\n",
    "\n",
    "        steps += 1\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    scores.append(score)\n",
    "    last10_mean = 0.\n",
    "    if i_episode > 10:\n",
    "        last10_mean = np.mean(scores[i_episode-10:])\n",
    "    print(\"\\rGame {:3d} Score {:.3f} completed in {:5d} steps eps {:.3f} last 10 avg {:.3f}\".format(\n",
    "        i_episode, score, steps, eps, last10_mean), end=\"\")\n",
    "    if i_episode % 10 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "v=torch.LongTensor([1])\n",
    "print(\"{:3d}\".format(v[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
